{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "58bc0f7d",
   "metadata": {},
   "source": [
    "# MRI based brain tumor IDH classification with MONAI (3D multiparametric MRI)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69d7c8f1",
   "metadata": {},
   "source": [
    "This tutorial shows how to construct a training workflow of binary classification task.  \n",
    "And it contains below features:\n",
    "1. Transforms for Monai dictionary format data.\n",
    "2. Define a new transform according MONAI transform API.\n",
    "3. Load Nifti image with metadata, load a list of images and stack them.\n",
    "5. 3D Voxel DynUNet model, Dice loss, cross entropy loss function for IDH classification task.\n",
    "6. Deterministic training for reproducibility.\n",
    "\n",
    "The Brain tumor dataset can be downloaded from \n",
    "https://ipp.cbica.upenn.edu/ and  http://medicaldecathlon.com/.  \n",
    "\n",
    "Target: IDH classification based on whole brain, tumour core, whole tumor, and enhancing tumor from MRI \n",
    "Modality: Multimodal multisite MRI data (FLAIR, T1w, T1gd,T2w)  \n",
    "training: 135 3D MRI \\\n",
    "validation:  \\\n",
    "testing: Not revealed\n",
    "\n",
    "Source: BRATS 2020/2021 datasets.  \n",
    "Challenge: RSNA-MICCAI Brain Tumor Radiogenomic Classification\n",
    "\n",
    "Below figure shows image patches with the tumor sub-regions that are annotated in the different modalities (top left) and the final labels for the whole dataset (right). (Figure taken from the [BraTS IEEE TMI paper](https://ieeexplore.ieee.org/document/6975210/))  \n",
    "![image](https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/42/7283692/6975210/6975210-fig-3-source-large.gif)\n",
    "\n",
    "The image patches show from left to right:\n",
    "1. the whole tumor (yellow) visible in T2-FLAIR (Fig.A).\n",
    "2. the tumor core (red) visible in T2 (Fig.B).\n",
    "3. the enhancing tumor structures (light blue) visible in T1Gd, surrounding the cystic/necrotic components of the core (green) (Fig. C).\n",
    "4. The segmentations are used to generate the final labels of the tumor sub-regions (Fig.D): edema (yellow), non-enhancing solid core (red), necrotic/cystic core (green), enhancing core (blue)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5073f899",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MONAI version: 0.8.0\n",
      "Numpy version: 1.20.2\n",
      "Pytorch version: 1.9.1\n",
      "MONAI flags: HAS_EXT = False, USE_COMPILED = False\n",
      "MONAI rev id: 714d00dffe6653e21260160666c4c201ab66511b\n",
      "\n",
      "Optional dependencies:\n",
      "Pytorch Ignite version: 0.4.4\n",
      "Nibabel version: 3.2.1\n",
      "scikit-image version: 0.18.1\n",
      "Pillow version: 8.2.0\n",
      "Tensorboard version: 2.5.0\n",
      "gdown version: 3.13.0\n",
      "TorchVision version: 0.10.1\n",
      "tqdm version: 4.61.0\n",
      "lmdb version: 1.2.1\n",
      "psutil version: 5.8.0\n",
      "pandas version: 1.2.4\n",
      "einops version: 0.3.0\n",
      "transformers version: NOT INSTALLED or UNKNOWN VERSION.\n",
      "mlflow version: NOT INSTALLED or UNKNOWN VERSION.\n",
      "\n",
      "For details about installing the optional dependencies, please visit:\n",
      "    https://docs.monai.io/en/latest/installation.html#installing-the-recommended-dependencies\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Copyright 2020 MONAI Consortium\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#     http://www.apache.org/licenses/LICENSE-2.0\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "\n",
    "import os\n",
    "import shutil\n",
    "import tempfile\n",
    "import sys\n",
    "import gc\n",
    "import logging\n",
    "import copy\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.axes_grid1 import ImageGrid\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy\n",
    "from scipy import ndimage\n",
    "from sklearn.model_selection import KFold\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from monai.networks.nets import DynUNet, EfficientNetBN, DenseNet121, SegResNet, SegResNetVAE\n",
    "from monai.data import CacheDataset, Dataset, DataLoader, ThreadDataLoader\n",
    "from torch.utils.data import WeightedRandomSampler\n",
    "\n",
    "import monai\n",
    "from monai.transforms import (\n",
    "    Activations,\n",
    "    AsDiscrete,\n",
    "    CastToTyped,\n",
    "    Compose, \n",
    "    CropForegroundd,\n",
    "    ResizeWithPadOrCrop,\n",
    "    ResizeWithPadOrCropd,\n",
    "    Spacingd,\n",
    "    RandRotate90d,\n",
    "    Resized,\n",
    "    EnsureChannelFirstd, \n",
    "    Orientationd,\n",
    "    LoadImaged,\n",
    "    NormalizeIntensity,\n",
    "    HistogramNormalize,\n",
    "    NormalizeIntensityd,\n",
    "    RandCropByPosNegLabeld,\n",
    "    RandCropByLabelClassesd,\n",
    "    RandAffined,\n",
    "    RandFlipd,\n",
    "    Flipd,\n",
    "    RandGaussianNoised,\n",
    "    RandGaussianSmoothd,\n",
    "    RandGibbsNoised,\n",
    "    RandStdShiftIntensityd,\n",
    "    RandScaleIntensityd,\n",
    "    RandZoomd, \n",
    "    SpatialCrop, \n",
    "    SpatialPadd, \n",
    "    MapTransform,\n",
    "    CastToType,\n",
    "    ToTensord,\n",
    "    AddChanneld,\n",
    "    MapTransform,\n",
    "    Orientationd,\n",
    "    ScaleIntensityd,\n",
    "    ScaleIntensity,\n",
    "    ScaleIntensityRangePercentilesd,\n",
    "    KeepLargestConnectedComponentd,\n",
    "    KeepLargestConnectedComponent,\n",
    "    ScaleIntensityRange,\n",
    "    RandShiftIntensityd,\n",
    "    RandAdjustContrastd,\n",
    "    AdjustContrastd,\n",
    "    Rotated,\n",
    "    ToNumpyd,\n",
    "    ToDeviced,\n",
    "    EnsureType,\n",
    "    EnsureTyped,\n",
    "    DataStatsd,\n",
    ")\n",
    "\n",
    "from monai.config import KeysCollection\n",
    "from monai.transforms.compose import MapTransform, Randomizable\n",
    "from collections.abc import Iterable\n",
    "from typing import Any, Dict, Hashable, Mapping, Optional, Sequence, Tuple, Union\n",
    "from monai.utils import set_determinism\n",
    "from monai.utils import (\n",
    "    ensure_tuple,\n",
    "    ensure_tuple_rep,\n",
    "    ensure_tuple_size,\n",
    ")\n",
    "\n",
    "from monai.optimizers import LearningRateFinder\n",
    "\n",
    "from monai.transforms.compose import MapTransform\n",
    "from monai.transforms.utils import generate_spatial_bounding_box\n",
    "from skimage.transform import resize\n",
    "from monai.losses import DiceCELoss, DiceLoss\n",
    "from monai.utils import set_determinism\n",
    "from monai.inferers import sliding_window_inference\n",
    "\n",
    "\n",
    "from monai.metrics import DiceMetric, ROCAUCMetric\n",
    "from monai.data import decollate_batch\n",
    "import glob\n",
    "import monai\n",
    "from monai.metrics import compute_meandice\n",
    "import random\n",
    "import pickle\n",
    "from collections import OrderedDict\n",
    "from typing import Sequence, Optional\n",
    "import ipywidgets as widgets\n",
    "from itertools import compress\n",
    "import SimpleITK as sitk\n",
    "import torchio as tio\n",
    "\n",
    "import sklearn\n",
    "from sklearn.metrics import mean_squared_error, roc_auc_score, accuracy_score, recall_score, \\\n",
    "accuracy_score, precision_score, f1_score, make_scorer \n",
    "\n",
    "from monai.utils import ensure_tuple_rep\n",
    "from monai.networks.layers.factories import Conv, Dropout, Norm, Pool\n",
    "import matplotlib.pyplot as plt\n",
    "from ranger21 import Ranger21\n",
    "\n",
    "### monai and ignite based imports\n",
    "import logging\n",
    "from ignite.engine import Engine, Events\n",
    "from ignite.contrib.handlers import FastaiLRFinder, ProgressBar\n",
    "from ignite.engine import (\n",
    "    Events,\n",
    "    _prepare_batch,\n",
    "    create_supervised_evaluator,\n",
    "    create_supervised_trainer,\n",
    ")\n",
    "from ignite.handlers import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "from itkwidgets import view\n",
    "import random\n",
    "monai.config.print_config()\n",
    "#from sliding_window_inference_classes import sliding_window_inference_classes"
   ]
  },
  {
   "cell_type": "raw",
   "id": "e82d161e",
   "metadata": {},
   "source": [
    "# Pipelines implemented here\n",
    "#[image](ProposedArchImgPath =250x250)\\\n",
    "<img src=\"assets/ProposedIDHClass.png\" align=\"left\" width=\"1024\" height=\"1800\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3ec3782",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4044cbb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_THREADS =2\n",
    "sitk.ProcessObject.SetGlobalDefaultNumberOfThreads(MAX_THREADS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5866b4ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sun Jun 12 17:30:57 2022       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 450.172.01   Driver Version: 450.172.01   CUDA Version: 11.0     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  Tesla V100-DGXS...  On   | 00000000:07:00.0 Off |                    0 |\n",
      "| N/A   43C    P0    43W / 300W |    108MiB / 32505MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   1  Tesla V100-DGXS...  On   | 00000000:08:00.0 Off |                    0 |\n",
      "| N/A   46C    P0    54W / 300W |   7832MiB / 32508MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   2  Tesla V100-DGXS...  On   | 00000000:0E:00.0 Off |                    0 |\n",
      "| N/A   45C    P0    52W / 300W |  13776MiB / 32508MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   3  Tesla V100-DGXS...  On   | 00000000:0F:00.0 Off |                    0 |\n",
      "| N/A   53C    P0   244W / 300W |  31646MiB / 32508MiB |    100%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|    0   N/A  N/A      2917      G   /usr/lib/xorg/Xorg                 86MiB |\n",
      "|    0   N/A  N/A      3284      G   /usr/bin/gnome-shell               16MiB |\n",
      "|    1   N/A  N/A      2917      G   /usr/lib/xorg/Xorg                  4MiB |\n",
      "|    2   N/A  N/A      2917      G   /usr/lib/xorg/Xorg                  4MiB |\n",
      "|    2   N/A  N/A   1426831      C   .../sa_tumorseg21/bin/python    13763MiB |\n",
      "|    3   N/A  N/A      2917      G   /usr/lib/xorg/Xorg                  4MiB |\n",
      "|    3   N/A  N/A   1427572      C   .../sa_tumorseg21/bin/python    31633MiB |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "seeds = 40961024\n",
    "set_determinism(seed=seeds)\n",
    "##np.random.seed(seeds) np random seed does not work here\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9d41897d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#patch_size = (128, 128, 128)\n",
    "spacing = (1.0, 1.0, 1.0)\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] =\"1\"\n",
    "device = torch.device('cuda:0')\n",
    "deviceName = 'cuda:0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1bff6de9",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_colwidth', None)\n",
    "data_rpath = '/home/mmiv-ml/data'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "21839313",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>BraTS2020</th>\n",
       "      <th>t1wPath</th>\n",
       "      <th>t1cwPath</th>\n",
       "      <th>t2wPath</th>\n",
       "      <th>flairPath</th>\n",
       "      <th>segPath</th>\n",
       "      <th>t1w_BrainmaskPath</th>\n",
       "      <th>IDH_value</th>\n",
       "      <th>BraTS2021</th>\n",
       "      <th>BraTS2019</th>\n",
       "      <th>...</th>\n",
       "      <th>ET_CoordX</th>\n",
       "      <th>ET_CoordY</th>\n",
       "      <th>ET_CoordZ</th>\n",
       "      <th>ED_CoordX</th>\n",
       "      <th>ED_CoordY</th>\n",
       "      <th>ED_CoordZ</th>\n",
       "      <th>NEC_CoordX</th>\n",
       "      <th>NEC_CoordY</th>\n",
       "      <th>NEC_CoordZ</th>\n",
       "      <th>is_merged_3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>BraTS20_Training_274</td>\n",
       "      <td>/home/mmiv-ml/data/MICCAI_BraTS2020_TrainingData/BraTS20_Training_274/BraTS20_Training_274_t1.nii.gz</td>\n",
       "      <td>/home/mmiv-ml/data/MICCAI_BraTS2020_TrainingData/BraTS20_Training_274/BraTS20_Training_274_t1ce.nii.gz</td>\n",
       "      <td>/home/mmiv-ml/data/MICCAI_BraTS2020_TrainingData/BraTS20_Training_274/BraTS20_Training_274_t2.nii.gz</td>\n",
       "      <td>/home/mmiv-ml/data/MICCAI_BraTS2020_TrainingData/BraTS20_Training_274/BraTS20_Training_274_flair.nii.gz</td>\n",
       "      <td>/home/mmiv-ml/data/MICCAI_BraTS2020_TrainingData/BraTS20_Training_274/BraTS20_Training_274_seg.nii.gz</td>\n",
       "      <td>/home/mmiv-ml/data/ROIBrain_MICCAI_BraTS2020/BraTS20_Training_274/BraTS20_Training_274_BrainROI.nii.gz</td>\n",
       "      <td>0</td>\n",
       "      <td>BraTS2021_01479</td>\n",
       "      <td>BraTS19_TCIA09_141_1</td>\n",
       "      <td>...</td>\n",
       "      <td>102.788589</td>\n",
       "      <td>97.639325</td>\n",
       "      <td>94.684363</td>\n",
       "      <td>118.038612</td>\n",
       "      <td>106.882949</td>\n",
       "      <td>88.290266</td>\n",
       "      <td>106.202255</td>\n",
       "      <td>91.666134</td>\n",
       "      <td>96.335216</td>\n",
       "      <td>both</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>BraTS20_Training_293</td>\n",
       "      <td>/home/mmiv-ml/data/MICCAI_BraTS2020_TrainingData/BraTS20_Training_293/BraTS20_Training_293_t1.nii.gz</td>\n",
       "      <td>/home/mmiv-ml/data/MICCAI_BraTS2020_TrainingData/BraTS20_Training_293/BraTS20_Training_293_t1ce.nii.gz</td>\n",
       "      <td>/home/mmiv-ml/data/MICCAI_BraTS2020_TrainingData/BraTS20_Training_293/BraTS20_Training_293_t2.nii.gz</td>\n",
       "      <td>/home/mmiv-ml/data/MICCAI_BraTS2020_TrainingData/BraTS20_Training_293/BraTS20_Training_293_flair.nii.gz</td>\n",
       "      <td>/home/mmiv-ml/data/MICCAI_BraTS2020_TrainingData/BraTS20_Training_293/BraTS20_Training_293_seg.nii.gz</td>\n",
       "      <td>/home/mmiv-ml/data/ROIBrain_MICCAI_BraTS2020/BraTS20_Training_293/BraTS20_Training_293_BrainROI.nii.gz</td>\n",
       "      <td>0</td>\n",
       "      <td>BraTS2021_01498</td>\n",
       "      <td>BraTS19_TCIA10_410_1</td>\n",
       "      <td>...</td>\n",
       "      <td>101.672477</td>\n",
       "      <td>85.079209</td>\n",
       "      <td>81.828045</td>\n",
       "      <td>96.746334</td>\n",
       "      <td>107.863478</td>\n",
       "      <td>83.794617</td>\n",
       "      <td>99.950014</td>\n",
       "      <td>88.945683</td>\n",
       "      <td>89.526921</td>\n",
       "      <td>both</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>BraTS20_Training_190</td>\n",
       "      <td>/home/mmiv-ml/data/MICCAI_BraTS2020_TrainingData/BraTS20_Training_190/BraTS20_Training_190_t1.nii.gz</td>\n",
       "      <td>/home/mmiv-ml/data/MICCAI_BraTS2020_TrainingData/BraTS20_Training_190/BraTS20_Training_190_t1ce.nii.gz</td>\n",
       "      <td>/home/mmiv-ml/data/MICCAI_BraTS2020_TrainingData/BraTS20_Training_190/BraTS20_Training_190_t2.nii.gz</td>\n",
       "      <td>/home/mmiv-ml/data/MICCAI_BraTS2020_TrainingData/BraTS20_Training_190/BraTS20_Training_190_flair.nii.gz</td>\n",
       "      <td>/home/mmiv-ml/data/MICCAI_BraTS2020_TrainingData/BraTS20_Training_190/BraTS20_Training_190_seg.nii.gz</td>\n",
       "      <td>/home/mmiv-ml/data/ROIBrain_MICCAI_BraTS2020/BraTS20_Training_190/BraTS20_Training_190_BrainROI.nii.gz</td>\n",
       "      <td>1</td>\n",
       "      <td>BraTS2021_01300</td>\n",
       "      <td>BraTS19_TCIA02_226_1</td>\n",
       "      <td>...</td>\n",
       "      <td>161.133893</td>\n",
       "      <td>117.501382</td>\n",
       "      <td>72.152114</td>\n",
       "      <td>158.558518</td>\n",
       "      <td>124.733076</td>\n",
       "      <td>69.336469</td>\n",
       "      <td>161.917178</td>\n",
       "      <td>114.233129</td>\n",
       "      <td>78.978528</td>\n",
       "      <td>both</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>BraTS20_Training_298</td>\n",
       "      <td>/home/mmiv-ml/data/MICCAI_BraTS2020_TrainingData/BraTS20_Training_298/BraTS20_Training_298_t1.nii.gz</td>\n",
       "      <td>/home/mmiv-ml/data/MICCAI_BraTS2020_TrainingData/BraTS20_Training_298/BraTS20_Training_298_t1ce.nii.gz</td>\n",
       "      <td>/home/mmiv-ml/data/MICCAI_BraTS2020_TrainingData/BraTS20_Training_298/BraTS20_Training_298_t2.nii.gz</td>\n",
       "      <td>/home/mmiv-ml/data/MICCAI_BraTS2020_TrainingData/BraTS20_Training_298/BraTS20_Training_298_flair.nii.gz</td>\n",
       "      <td>/home/mmiv-ml/data/MICCAI_BraTS2020_TrainingData/BraTS20_Training_298/BraTS20_Training_298_seg.nii.gz</td>\n",
       "      <td>/home/mmiv-ml/data/ROIBrain_MICCAI_BraTS2020/BraTS20_Training_298/BraTS20_Training_298_BrainROI.nii.gz</td>\n",
       "      <td>0</td>\n",
       "      <td>BraTS2021_01503</td>\n",
       "      <td>BraTS19_TCIA10_276_1</td>\n",
       "      <td>...</td>\n",
       "      <td>110.542553</td>\n",
       "      <td>73.074468</td>\n",
       "      <td>70.808511</td>\n",
       "      <td>107.090113</td>\n",
       "      <td>82.676138</td>\n",
       "      <td>76.029439</td>\n",
       "      <td>105.099771</td>\n",
       "      <td>65.077985</td>\n",
       "      <td>76.992437</td>\n",
       "      <td>both</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>BraTS20_Training_334</td>\n",
       "      <td>/home/mmiv-ml/data/MICCAI_BraTS2020_TrainingData/BraTS20_Training_334/BraTS20_Training_334_t1.nii.gz</td>\n",
       "      <td>/home/mmiv-ml/data/MICCAI_BraTS2020_TrainingData/BraTS20_Training_334/BraTS20_Training_334_t1ce.nii.gz</td>\n",
       "      <td>/home/mmiv-ml/data/MICCAI_BraTS2020_TrainingData/BraTS20_Training_334/BraTS20_Training_334_t2.nii.gz</td>\n",
       "      <td>/home/mmiv-ml/data/MICCAI_BraTS2020_TrainingData/BraTS20_Training_334/BraTS20_Training_334_flair.nii.gz</td>\n",
       "      <td>/home/mmiv-ml/data/MICCAI_BraTS2020_TrainingData/BraTS20_Training_334/BraTS20_Training_334_seg.nii.gz</td>\n",
       "      <td>/home/mmiv-ml/data/ROIBrain_MICCAI_BraTS2020/BraTS20_Training_334/BraTS20_Training_334_BrainROI.nii.gz</td>\n",
       "      <td>0</td>\n",
       "      <td>BraTS2021_01665</td>\n",
       "      <td>BraTS19_TCIA13_624_1</td>\n",
       "      <td>...</td>\n",
       "      <td>81.914264</td>\n",
       "      <td>146.606787</td>\n",
       "      <td>69.382751</td>\n",
       "      <td>80.744102</td>\n",
       "      <td>138.641146</td>\n",
       "      <td>66.538335</td>\n",
       "      <td>80.554348</td>\n",
       "      <td>144.624506</td>\n",
       "      <td>74.856719</td>\n",
       "      <td>both</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>130</th>\n",
       "      <td>BraTS20_Training_234</td>\n",
       "      <td>/home/mmiv-ml/data/MICCAI_BraTS2020_TrainingData/BraTS20_Training_234/BraTS20_Training_234_t1.nii.gz</td>\n",
       "      <td>/home/mmiv-ml/data/MICCAI_BraTS2020_TrainingData/BraTS20_Training_234/BraTS20_Training_234_t1ce.nii.gz</td>\n",
       "      <td>/home/mmiv-ml/data/MICCAI_BraTS2020_TrainingData/BraTS20_Training_234/BraTS20_Training_234_t2.nii.gz</td>\n",
       "      <td>/home/mmiv-ml/data/MICCAI_BraTS2020_TrainingData/BraTS20_Training_234/BraTS20_Training_234_flair.nii.gz</td>\n",
       "      <td>/home/mmiv-ml/data/MICCAI_BraTS2020_TrainingData/BraTS20_Training_234/BraTS20_Training_234_seg.nii.gz</td>\n",
       "      <td>/home/mmiv-ml/data/ROIBrain_MICCAI_BraTS2020/BraTS20_Training_234/BraTS20_Training_234_BrainROI.nii.gz</td>\n",
       "      <td>0</td>\n",
       "      <td>BraTS2021_00134</td>\n",
       "      <td>BraTS19_TCIA05_444_1</td>\n",
       "      <td>...</td>\n",
       "      <td>135.527764</td>\n",
       "      <td>82.683842</td>\n",
       "      <td>82.832916</td>\n",
       "      <td>139.213709</td>\n",
       "      <td>94.138320</td>\n",
       "      <td>68.037037</td>\n",
       "      <td>136.153603</td>\n",
       "      <td>95.470173</td>\n",
       "      <td>68.157774</td>\n",
       "      <td>both</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131</th>\n",
       "      <td>BraTS20_Training_249</td>\n",
       "      <td>/home/mmiv-ml/data/MICCAI_BraTS2020_TrainingData/BraTS20_Training_249/BraTS20_Training_249_t1.nii.gz</td>\n",
       "      <td>/home/mmiv-ml/data/MICCAI_BraTS2020_TrainingData/BraTS20_Training_249/BraTS20_Training_249_t1ce.nii.gz</td>\n",
       "      <td>/home/mmiv-ml/data/MICCAI_BraTS2020_TrainingData/BraTS20_Training_249/BraTS20_Training_249_t2.nii.gz</td>\n",
       "      <td>/home/mmiv-ml/data/MICCAI_BraTS2020_TrainingData/BraTS20_Training_249/BraTS20_Training_249_flair.nii.gz</td>\n",
       "      <td>/home/mmiv-ml/data/MICCAI_BraTS2020_TrainingData/BraTS20_Training_249/BraTS20_Training_249_seg.nii.gz</td>\n",
       "      <td>/home/mmiv-ml/data/ROIBrain_MICCAI_BraTS2020/BraTS20_Training_249/BraTS20_Training_249_BrainROI.nii.gz</td>\n",
       "      <td>1</td>\n",
       "      <td>BraTS2021_00148</td>\n",
       "      <td>BraTS19_TCIA08_280_1</td>\n",
       "      <td>...</td>\n",
       "      <td>167.902679</td>\n",
       "      <td>124.229911</td>\n",
       "      <td>48.055506</td>\n",
       "      <td>161.954812</td>\n",
       "      <td>120.572396</td>\n",
       "      <td>53.495751</td>\n",
       "      <td>165.754088</td>\n",
       "      <td>122.700629</td>\n",
       "      <td>53.863522</td>\n",
       "      <td>both</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>132</th>\n",
       "      <td>BraTS20_Training_171</td>\n",
       "      <td>/home/mmiv-ml/data/MICCAI_BraTS2020_TrainingData/BraTS20_Training_171/BraTS20_Training_171_t1.nii.gz</td>\n",
       "      <td>/home/mmiv-ml/data/MICCAI_BraTS2020_TrainingData/BraTS20_Training_171/BraTS20_Training_171_t1ce.nii.gz</td>\n",
       "      <td>/home/mmiv-ml/data/MICCAI_BraTS2020_TrainingData/BraTS20_Training_171/BraTS20_Training_171_t2.nii.gz</td>\n",
       "      <td>/home/mmiv-ml/data/MICCAI_BraTS2020_TrainingData/BraTS20_Training_171/BraTS20_Training_171_flair.nii.gz</td>\n",
       "      <td>/home/mmiv-ml/data/MICCAI_BraTS2020_TrainingData/BraTS20_Training_171/BraTS20_Training_171_seg.nii.gz</td>\n",
       "      <td>/home/mmiv-ml/data/ROIBrain_MICCAI_BraTS2020/BraTS20_Training_171/BraTS20_Training_171_BrainROI.nii.gz</td>\n",
       "      <td>1</td>\n",
       "      <td>BraTS2021_00999</td>\n",
       "      <td>BraTS19_TCIA01_180_1</td>\n",
       "      <td>...</td>\n",
       "      <td>104.061792</td>\n",
       "      <td>111.841124</td>\n",
       "      <td>95.391230</td>\n",
       "      <td>106.538165</td>\n",
       "      <td>100.652189</td>\n",
       "      <td>97.482928</td>\n",
       "      <td>102.660285</td>\n",
       "      <td>106.012682</td>\n",
       "      <td>93.015594</td>\n",
       "      <td>both</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>133</th>\n",
       "      <td>BraTS20_Training_185</td>\n",
       "      <td>/home/mmiv-ml/data/MICCAI_BraTS2020_TrainingData/BraTS20_Training_185/BraTS20_Training_185_t1.nii.gz</td>\n",
       "      <td>/home/mmiv-ml/data/MICCAI_BraTS2020_TrainingData/BraTS20_Training_185/BraTS20_Training_185_t1ce.nii.gz</td>\n",
       "      <td>/home/mmiv-ml/data/MICCAI_BraTS2020_TrainingData/BraTS20_Training_185/BraTS20_Training_185_t2.nii.gz</td>\n",
       "      <td>/home/mmiv-ml/data/MICCAI_BraTS2020_TrainingData/BraTS20_Training_185/BraTS20_Training_185_flair.nii.gz</td>\n",
       "      <td>/home/mmiv-ml/data/MICCAI_BraTS2020_TrainingData/BraTS20_Training_185/BraTS20_Training_185_seg.nii.gz</td>\n",
       "      <td>/home/mmiv-ml/data/ROIBrain_MICCAI_BraTS2020/BraTS20_Training_185/BraTS20_Training_185_BrainROI.nii.gz</td>\n",
       "      <td>1</td>\n",
       "      <td>BraTS2021_00142</td>\n",
       "      <td>BraTS19_TCIA02_314_1</td>\n",
       "      <td>...</td>\n",
       "      <td>145.337610</td>\n",
       "      <td>127.855479</td>\n",
       "      <td>61.117421</td>\n",
       "      <td>145.918486</td>\n",
       "      <td>133.762711</td>\n",
       "      <td>67.759784</td>\n",
       "      <td>145.855751</td>\n",
       "      <td>132.161974</td>\n",
       "      <td>59.734923</td>\n",
       "      <td>both</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>134</th>\n",
       "      <td>BraTS20_Training_286</td>\n",
       "      <td>/home/mmiv-ml/data/MICCAI_BraTS2020_TrainingData/BraTS20_Training_286/BraTS20_Training_286_t1.nii.gz</td>\n",
       "      <td>/home/mmiv-ml/data/MICCAI_BraTS2020_TrainingData/BraTS20_Training_286/BraTS20_Training_286_t1ce.nii.gz</td>\n",
       "      <td>/home/mmiv-ml/data/MICCAI_BraTS2020_TrainingData/BraTS20_Training_286/BraTS20_Training_286_t2.nii.gz</td>\n",
       "      <td>/home/mmiv-ml/data/MICCAI_BraTS2020_TrainingData/BraTS20_Training_286/BraTS20_Training_286_flair.nii.gz</td>\n",
       "      <td>/home/mmiv-ml/data/MICCAI_BraTS2020_TrainingData/BraTS20_Training_286/BraTS20_Training_286_seg.nii.gz</td>\n",
       "      <td>/home/mmiv-ml/data/ROIBrain_MICCAI_BraTS2020/BraTS20_Training_286/BraTS20_Training_286_BrainROI.nii.gz</td>\n",
       "      <td>0</td>\n",
       "      <td>BraTS2021_01491</td>\n",
       "      <td>BraTS19_TCIA10_266_1</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>135.505986</td>\n",
       "      <td>77.634155</td>\n",
       "      <td>93.614085</td>\n",
       "      <td>131.696915</td>\n",
       "      <td>74.564369</td>\n",
       "      <td>86.741668</td>\n",
       "      <td>both</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>135 rows Ã— 48 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                BraTS2020  \\\n",
       "0    BraTS20_Training_274   \n",
       "1    BraTS20_Training_293   \n",
       "2    BraTS20_Training_190   \n",
       "3    BraTS20_Training_298   \n",
       "4    BraTS20_Training_334   \n",
       "..                    ...   \n",
       "130  BraTS20_Training_234   \n",
       "131  BraTS20_Training_249   \n",
       "132  BraTS20_Training_171   \n",
       "133  BraTS20_Training_185   \n",
       "134  BraTS20_Training_286   \n",
       "\n",
       "                                                                                                  t1wPath  \\\n",
       "0    /home/mmiv-ml/data/MICCAI_BraTS2020_TrainingData/BraTS20_Training_274/BraTS20_Training_274_t1.nii.gz   \n",
       "1    /home/mmiv-ml/data/MICCAI_BraTS2020_TrainingData/BraTS20_Training_293/BraTS20_Training_293_t1.nii.gz   \n",
       "2    /home/mmiv-ml/data/MICCAI_BraTS2020_TrainingData/BraTS20_Training_190/BraTS20_Training_190_t1.nii.gz   \n",
       "3    /home/mmiv-ml/data/MICCAI_BraTS2020_TrainingData/BraTS20_Training_298/BraTS20_Training_298_t1.nii.gz   \n",
       "4    /home/mmiv-ml/data/MICCAI_BraTS2020_TrainingData/BraTS20_Training_334/BraTS20_Training_334_t1.nii.gz   \n",
       "..                                                                                                    ...   \n",
       "130  /home/mmiv-ml/data/MICCAI_BraTS2020_TrainingData/BraTS20_Training_234/BraTS20_Training_234_t1.nii.gz   \n",
       "131  /home/mmiv-ml/data/MICCAI_BraTS2020_TrainingData/BraTS20_Training_249/BraTS20_Training_249_t1.nii.gz   \n",
       "132  /home/mmiv-ml/data/MICCAI_BraTS2020_TrainingData/BraTS20_Training_171/BraTS20_Training_171_t1.nii.gz   \n",
       "133  /home/mmiv-ml/data/MICCAI_BraTS2020_TrainingData/BraTS20_Training_185/BraTS20_Training_185_t1.nii.gz   \n",
       "134  /home/mmiv-ml/data/MICCAI_BraTS2020_TrainingData/BraTS20_Training_286/BraTS20_Training_286_t1.nii.gz   \n",
       "\n",
       "                                                                                                   t1cwPath  \\\n",
       "0    /home/mmiv-ml/data/MICCAI_BraTS2020_TrainingData/BraTS20_Training_274/BraTS20_Training_274_t1ce.nii.gz   \n",
       "1    /home/mmiv-ml/data/MICCAI_BraTS2020_TrainingData/BraTS20_Training_293/BraTS20_Training_293_t1ce.nii.gz   \n",
       "2    /home/mmiv-ml/data/MICCAI_BraTS2020_TrainingData/BraTS20_Training_190/BraTS20_Training_190_t1ce.nii.gz   \n",
       "3    /home/mmiv-ml/data/MICCAI_BraTS2020_TrainingData/BraTS20_Training_298/BraTS20_Training_298_t1ce.nii.gz   \n",
       "4    /home/mmiv-ml/data/MICCAI_BraTS2020_TrainingData/BraTS20_Training_334/BraTS20_Training_334_t1ce.nii.gz   \n",
       "..                                                                                                      ...   \n",
       "130  /home/mmiv-ml/data/MICCAI_BraTS2020_TrainingData/BraTS20_Training_234/BraTS20_Training_234_t1ce.nii.gz   \n",
       "131  /home/mmiv-ml/data/MICCAI_BraTS2020_TrainingData/BraTS20_Training_249/BraTS20_Training_249_t1ce.nii.gz   \n",
       "132  /home/mmiv-ml/data/MICCAI_BraTS2020_TrainingData/BraTS20_Training_171/BraTS20_Training_171_t1ce.nii.gz   \n",
       "133  /home/mmiv-ml/data/MICCAI_BraTS2020_TrainingData/BraTS20_Training_185/BraTS20_Training_185_t1ce.nii.gz   \n",
       "134  /home/mmiv-ml/data/MICCAI_BraTS2020_TrainingData/BraTS20_Training_286/BraTS20_Training_286_t1ce.nii.gz   \n",
       "\n",
       "                                                                                                  t2wPath  \\\n",
       "0    /home/mmiv-ml/data/MICCAI_BraTS2020_TrainingData/BraTS20_Training_274/BraTS20_Training_274_t2.nii.gz   \n",
       "1    /home/mmiv-ml/data/MICCAI_BraTS2020_TrainingData/BraTS20_Training_293/BraTS20_Training_293_t2.nii.gz   \n",
       "2    /home/mmiv-ml/data/MICCAI_BraTS2020_TrainingData/BraTS20_Training_190/BraTS20_Training_190_t2.nii.gz   \n",
       "3    /home/mmiv-ml/data/MICCAI_BraTS2020_TrainingData/BraTS20_Training_298/BraTS20_Training_298_t2.nii.gz   \n",
       "4    /home/mmiv-ml/data/MICCAI_BraTS2020_TrainingData/BraTS20_Training_334/BraTS20_Training_334_t2.nii.gz   \n",
       "..                                                                                                    ...   \n",
       "130  /home/mmiv-ml/data/MICCAI_BraTS2020_TrainingData/BraTS20_Training_234/BraTS20_Training_234_t2.nii.gz   \n",
       "131  /home/mmiv-ml/data/MICCAI_BraTS2020_TrainingData/BraTS20_Training_249/BraTS20_Training_249_t2.nii.gz   \n",
       "132  /home/mmiv-ml/data/MICCAI_BraTS2020_TrainingData/BraTS20_Training_171/BraTS20_Training_171_t2.nii.gz   \n",
       "133  /home/mmiv-ml/data/MICCAI_BraTS2020_TrainingData/BraTS20_Training_185/BraTS20_Training_185_t2.nii.gz   \n",
       "134  /home/mmiv-ml/data/MICCAI_BraTS2020_TrainingData/BraTS20_Training_286/BraTS20_Training_286_t2.nii.gz   \n",
       "\n",
       "                                                                                                   flairPath  \\\n",
       "0    /home/mmiv-ml/data/MICCAI_BraTS2020_TrainingData/BraTS20_Training_274/BraTS20_Training_274_flair.nii.gz   \n",
       "1    /home/mmiv-ml/data/MICCAI_BraTS2020_TrainingData/BraTS20_Training_293/BraTS20_Training_293_flair.nii.gz   \n",
       "2    /home/mmiv-ml/data/MICCAI_BraTS2020_TrainingData/BraTS20_Training_190/BraTS20_Training_190_flair.nii.gz   \n",
       "3    /home/mmiv-ml/data/MICCAI_BraTS2020_TrainingData/BraTS20_Training_298/BraTS20_Training_298_flair.nii.gz   \n",
       "4    /home/mmiv-ml/data/MICCAI_BraTS2020_TrainingData/BraTS20_Training_334/BraTS20_Training_334_flair.nii.gz   \n",
       "..                                                                                                       ...   \n",
       "130  /home/mmiv-ml/data/MICCAI_BraTS2020_TrainingData/BraTS20_Training_234/BraTS20_Training_234_flair.nii.gz   \n",
       "131  /home/mmiv-ml/data/MICCAI_BraTS2020_TrainingData/BraTS20_Training_249/BraTS20_Training_249_flair.nii.gz   \n",
       "132  /home/mmiv-ml/data/MICCAI_BraTS2020_TrainingData/BraTS20_Training_171/BraTS20_Training_171_flair.nii.gz   \n",
       "133  /home/mmiv-ml/data/MICCAI_BraTS2020_TrainingData/BraTS20_Training_185/BraTS20_Training_185_flair.nii.gz   \n",
       "134  /home/mmiv-ml/data/MICCAI_BraTS2020_TrainingData/BraTS20_Training_286/BraTS20_Training_286_flair.nii.gz   \n",
       "\n",
       "                                                                                                   segPath  \\\n",
       "0    /home/mmiv-ml/data/MICCAI_BraTS2020_TrainingData/BraTS20_Training_274/BraTS20_Training_274_seg.nii.gz   \n",
       "1    /home/mmiv-ml/data/MICCAI_BraTS2020_TrainingData/BraTS20_Training_293/BraTS20_Training_293_seg.nii.gz   \n",
       "2    /home/mmiv-ml/data/MICCAI_BraTS2020_TrainingData/BraTS20_Training_190/BraTS20_Training_190_seg.nii.gz   \n",
       "3    /home/mmiv-ml/data/MICCAI_BraTS2020_TrainingData/BraTS20_Training_298/BraTS20_Training_298_seg.nii.gz   \n",
       "4    /home/mmiv-ml/data/MICCAI_BraTS2020_TrainingData/BraTS20_Training_334/BraTS20_Training_334_seg.nii.gz   \n",
       "..                                                                                                     ...   \n",
       "130  /home/mmiv-ml/data/MICCAI_BraTS2020_TrainingData/BraTS20_Training_234/BraTS20_Training_234_seg.nii.gz   \n",
       "131  /home/mmiv-ml/data/MICCAI_BraTS2020_TrainingData/BraTS20_Training_249/BraTS20_Training_249_seg.nii.gz   \n",
       "132  /home/mmiv-ml/data/MICCAI_BraTS2020_TrainingData/BraTS20_Training_171/BraTS20_Training_171_seg.nii.gz   \n",
       "133  /home/mmiv-ml/data/MICCAI_BraTS2020_TrainingData/BraTS20_Training_185/BraTS20_Training_185_seg.nii.gz   \n",
       "134  /home/mmiv-ml/data/MICCAI_BraTS2020_TrainingData/BraTS20_Training_286/BraTS20_Training_286_seg.nii.gz   \n",
       "\n",
       "                                                                                          t1w_BrainmaskPath  \\\n",
       "0    /home/mmiv-ml/data/ROIBrain_MICCAI_BraTS2020/BraTS20_Training_274/BraTS20_Training_274_BrainROI.nii.gz   \n",
       "1    /home/mmiv-ml/data/ROIBrain_MICCAI_BraTS2020/BraTS20_Training_293/BraTS20_Training_293_BrainROI.nii.gz   \n",
       "2    /home/mmiv-ml/data/ROIBrain_MICCAI_BraTS2020/BraTS20_Training_190/BraTS20_Training_190_BrainROI.nii.gz   \n",
       "3    /home/mmiv-ml/data/ROIBrain_MICCAI_BraTS2020/BraTS20_Training_298/BraTS20_Training_298_BrainROI.nii.gz   \n",
       "4    /home/mmiv-ml/data/ROIBrain_MICCAI_BraTS2020/BraTS20_Training_334/BraTS20_Training_334_BrainROI.nii.gz   \n",
       "..                                                                                                      ...   \n",
       "130  /home/mmiv-ml/data/ROIBrain_MICCAI_BraTS2020/BraTS20_Training_234/BraTS20_Training_234_BrainROI.nii.gz   \n",
       "131  /home/mmiv-ml/data/ROIBrain_MICCAI_BraTS2020/BraTS20_Training_249/BraTS20_Training_249_BrainROI.nii.gz   \n",
       "132  /home/mmiv-ml/data/ROIBrain_MICCAI_BraTS2020/BraTS20_Training_171/BraTS20_Training_171_BrainROI.nii.gz   \n",
       "133  /home/mmiv-ml/data/ROIBrain_MICCAI_BraTS2020/BraTS20_Training_185/BraTS20_Training_185_BrainROI.nii.gz   \n",
       "134  /home/mmiv-ml/data/ROIBrain_MICCAI_BraTS2020/BraTS20_Training_286/BraTS20_Training_286_BrainROI.nii.gz   \n",
       "\n",
       "     IDH_value        BraTS2021             BraTS2019  ...   ET_CoordX  \\\n",
       "0            0  BraTS2021_01479  BraTS19_TCIA09_141_1  ...  102.788589   \n",
       "1            0  BraTS2021_01498  BraTS19_TCIA10_410_1  ...  101.672477   \n",
       "2            1  BraTS2021_01300  BraTS19_TCIA02_226_1  ...  161.133893   \n",
       "3            0  BraTS2021_01503  BraTS19_TCIA10_276_1  ...  110.542553   \n",
       "4            0  BraTS2021_01665  BraTS19_TCIA13_624_1  ...   81.914264   \n",
       "..         ...              ...                   ...  ...         ...   \n",
       "130          0  BraTS2021_00134  BraTS19_TCIA05_444_1  ...  135.527764   \n",
       "131          1  BraTS2021_00148  BraTS19_TCIA08_280_1  ...  167.902679   \n",
       "132          1  BraTS2021_00999  BraTS19_TCIA01_180_1  ...  104.061792   \n",
       "133          1  BraTS2021_00142  BraTS19_TCIA02_314_1  ...  145.337610   \n",
       "134          0  BraTS2021_01491  BraTS19_TCIA10_266_1  ...         NaN   \n",
       "\n",
       "      ET_CoordY  ET_CoordZ   ED_CoordX   ED_CoordY  ED_CoordZ  NEC_CoordX  \\\n",
       "0     97.639325  94.684363  118.038612  106.882949  88.290266  106.202255   \n",
       "1     85.079209  81.828045   96.746334  107.863478  83.794617   99.950014   \n",
       "2    117.501382  72.152114  158.558518  124.733076  69.336469  161.917178   \n",
       "3     73.074468  70.808511  107.090113   82.676138  76.029439  105.099771   \n",
       "4    146.606787  69.382751   80.744102  138.641146  66.538335   80.554348   \n",
       "..          ...        ...         ...         ...        ...         ...   \n",
       "130   82.683842  82.832916  139.213709   94.138320  68.037037  136.153603   \n",
       "131  124.229911  48.055506  161.954812  120.572396  53.495751  165.754088   \n",
       "132  111.841124  95.391230  106.538165  100.652189  97.482928  102.660285   \n",
       "133  127.855479  61.117421  145.918486  133.762711  67.759784  145.855751   \n",
       "134         NaN        NaN  135.505986   77.634155  93.614085  131.696915   \n",
       "\n",
       "     NEC_CoordY NEC_CoordZ is_merged_3  \n",
       "0     91.666134  96.335216        both  \n",
       "1     88.945683  89.526921        both  \n",
       "2    114.233129  78.978528        both  \n",
       "3     65.077985  76.992437        both  \n",
       "4    144.624506  74.856719        both  \n",
       "..          ...        ...         ...  \n",
       "130   95.470173  68.157774        both  \n",
       "131  122.700629  53.863522        both  \n",
       "132  106.012682  93.015594        both  \n",
       "133  132.161974  59.734923        both  \n",
       "134   74.564369  86.741668        both  \n",
       "\n",
       "[135 rows x 48 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BraTS20SubjectsIDHWithMetaDF  = pd.read_csv('assets/BraTS20SubjectsIDHWithMetaDF.csv')\n",
    "BraTS20SubjectsIDHWithMetaDF"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74920578",
   "metadata": {},
   "source": [
    "## Cearing a list of dictionaries in order to feed into Monai's Dataset\n",
    "Keys:\n",
    "- ***image:*** T1, T1c, T2, and flair image\n",
    "- ***label:*** Segmented mask GT\n",
    "- ***brain_mask:*** Whole brain area (brain area=1 and Non brain area=0)\n",
    "- ***IDH_value:*** IDH class corresponding to the subject/images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "005fb0a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_files = [{'image': (image_nameT1, image_nameT1ce, image_nameT2, image_nameFl), 'label': label_name, 'brain_mask':brain_mask, 'IDH_label': np.array(IDH_label_name)} \n",
    "               for image_nameT1,image_nameT1ce, image_nameT2, image_nameFl, label_name, brain_mask, IDH_label_name \n",
    "               in zip(BraTS20SubjectsIDHWithMetaDF['t1wPath'], BraTS20SubjectsIDHWithMetaDF['t1cwPath'], BraTS20SubjectsIDHWithMetaDF['t2wPath'], BraTS20SubjectsIDHWithMetaDF['flairPath'],\\\n",
    "                     BraTS20SubjectsIDHWithMetaDF['segPath'], BraTS20SubjectsIDHWithMetaDF['t1w_BrainmaskPath'], BraTS20SubjectsIDHWithMetaDF['IDH_value'])]\n",
    "\n",
    "# train_files_image = [(image_nameT1, image_nameT1ce, image_nameT2, image_nameFl) \n",
    "#                      for image_nameT1,image_nameT1ce, image_nameT2, image_nameFl \n",
    "#                      in zip(dfTrainLbl['t1wPath'], dfTrainLbl['t1cwPath'], dfTrainLbl['T2wPath'], dfTrainLbl['FlairPath'])]\n",
    "# train_files_label = dfTrainLbl['segPath'].tolist()\n",
    "# train_files_brain_mask = dfTrainLbl['brain_maskPath'].tolist()\n",
    "# train_files_IDH_label = dfTrainLbl['IDH_value'].values.ravel().tolist()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2bf8c6f",
   "metadata": {},
   "source": [
    "## Creating 5 splits for cross validaion (5 cross validaion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bf1cebcf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 45 45 45\n",
      "val:  45 train:  90 \n",
      "\n",
      "val:  45 train:  90 \n",
      "\n",
      "val:  45 train:  90 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "n_splits = 3\n",
    "#train_index = np.linspace(0, train_features.shape[0]-1, num = train_features.shape[0], dtype = np.uint16, endpoint=True)\n",
    "#partition_data = monai.data.utils.partition_dataset_classes(train_index, train_labels.values.ravel().tolist(), shuffle=True, num_partitions=n_splits) \n",
    "#partition_data = monai.data.utils.partition_dataset_classes(train_files, dfTrainLbl['IDH_value'].values.ravel().tolist(), shuffle=True, num_partitions=n_splits)\n",
    "partition_data = monai.data.partition_dataset_classes(train_files, BraTS20SubjectsIDHWithMetaDF['IDH_value'].values.ravel().tolist(), shuffle=True, num_partitions=n_splits)\n",
    "print(len(partition_data), len(partition_data[0]), len(partition_data[1]), len(partition_data[2]))\n",
    "\n",
    "\n",
    "# val_folds = {}\n",
    "# train_folds = {}\n",
    "# flds = np.linspace(0, n_splits, num=n_splits, dtype = np.int8)\n",
    "# for cfold in range(n_splits):\n",
    "#     not_cfold = np.delete(flds, cfold)\n",
    "#     val_folds[cfold] = partition_data[cfold]\n",
    "# #     train_folds[cfold] = \n",
    "# # sub_flds = flds[..., ~0]   \n",
    "# # sub_flds\n",
    "\n",
    "val_folds = {}\n",
    "train_folds = {}\n",
    "flds = np.linspace(0, n_splits, num=n_splits, dtype = np.uint8)\n",
    "for cfold in range(n_splits):\n",
    "    #val_folds[f\"fold{cfold}\"] = train_features.values[partition_data[cfold],:]\n",
    "    #train_folds[f\"fold{cfold}\"] = np.delete(train_features.values, partition_data[cfold], axis=0)\n",
    "    #not_cfold = np.delete(flds, cfold)\n",
    "    \n",
    "    val_folds[f\"fold{cfold}\"] = partition_data[cfold]\n",
    "    val_folds[f\"fold{cfold}_IDH_label\"] = [adct['IDH_label'].item() for adct in partition_data[cfold]]\n",
    "    train_folds_masks = [1]*n_splits\n",
    "    train_folds_masks[cfold] = 0\n",
    "    partition_data_non_cfold = list()\n",
    "    for aDctLstitem in compress(partition_data, train_folds_masks):\n",
    "        partition_data_non_cfold.extend(aDctLstitem)\n",
    "        \n",
    "        \n",
    "    train_folds[f\"fold{cfold}\"] = partition_data_non_cfold\n",
    "    train_folds[f\"fold{cfold}_IDH_label\"] = [adct['IDH_label'].item() for adct in partition_data_non_cfold]\n",
    "\n",
    "for i in range(n_splits):\n",
    "    print('val: ', len(val_folds[f'fold{i}']), 'train: ', len(train_folds[f'fold{i}']), '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dadda4ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training classes\n",
      "\n",
      "(array([0, 1]), array([38, 52]))\n",
      "\n",
      "Validation classes\n",
      "\n",
      "(array([0, 1]), array([19, 26]))\n",
      "#### \n",
      "\n",
      "\n",
      "Training classes\n",
      "\n",
      "(array([0, 1]), array([38, 52]))\n",
      "\n",
      "Validation classes\n",
      "\n",
      "(array([0, 1]), array([19, 26]))\n",
      "#### \n",
      "\n",
      "\n",
      "Training classes\n",
      "\n",
      "(array([0, 1]), array([38, 52]))\n",
      "\n",
      "Validation classes\n",
      "\n",
      "(array([0, 1]), array([19, 26]))\n",
      "#### \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for i_cv in range(n_splits):\n",
    "    print('Training classes\\n')\n",
    "    print(np.unique([train_folds[f'fold{i_cv}'][i]['IDH_label'].item() for i in range(len(train_folds[f'fold{i_cv}']))], return_counts = True))\n",
    "    print('\\nValidation classes\\n')\n",
    "    print(np.unique([val_folds[f'fold{i_cv}'][i]['IDH_label'].item() for i in range(len(val_folds[f'fold{i_cv}']))], return_counts = True))\n",
    "    print('#'*4, '\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db7d9877",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "04363b44",
   "metadata": {},
   "source": [
    "***HistogramStandardization***\n",
    "\n",
    "Implementing histogram standardization from [torchIO](https://github.com/fepegar/torchio) library\n",
    "\n",
    "Bases: [torchio.transforms.preprocessing.intensity.normalization_transform.NormalizationTransform](https://torchio.readthedocs.io/transforms/preprocessing.html#torchio.transforms.preprocessing.intensity.NormalizationTransform)\n",
    "\n",
    "Perform histogram standardization of intensity values.\n",
    "\n",
    "Implementation of [New variants of a method of MRI scale standardization](https://ieeexplore.ieee.org/document/836373).\n",
    "\n",
    "We can visit in [torchio.transforms.HistogramStandardization.train()]((https://torchio.readthedocs.io/transforms/preprocessing.html#torchio.transforms.HistogramStandardization.train)) for more details.\n",
    "\n",
    "PARAMETERS\n",
    "landmarks â€“ Dictionary (or path to a PyTorch file with .pt or .pth extension in which a dictionary has been saved) whose keys are image names in the subject and values are NumPy arrays or paths to NumPy arrays defining the landmarks after training with [torchio.transforms.HistogramStandardization.train()](https://torchio.readthedocs.io/transforms/preprocessing.html#torchio.transforms.HistogramStandardization.train).\n",
    "\n",
    "Here, ***save_dir*** is a path where the trained histogram files for four channels (T1w, T1cw, T2w, and Flair), and trained model's weights will be saved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a2efa104",
   "metadata": {},
   "outputs": [],
   "source": [
    "# file_prefix = 'ConvEffNet_Brats21_1SplitV0'\n",
    "# savedirname = 'ConvEffNet_Brats21'\n",
    "# save_dir = os.path.join('/raid/brats2021/pthBraTS2021Radiogenomics', savedirname)\n",
    "# if not os.path.exists(save_dir):\n",
    "#     os.makedirs(save_dir)\n",
    "\n",
    "# train_images20T1 = dfTrainLbl['t1wPath'].values\n",
    "# train_images20T1ce = dfTrainLbl['t1cwPath'].values\n",
    "# train_images20T2 = dfTrainLbl['T2wPath'].values\n",
    "# train_images20Flair = dfTrainLbl['FlairPath'].values\n",
    "\n",
    "# hiseq_t1npyfile = os.path.join(save_dir, f\"histeq_t1w_{file_prefix}.npy\")\n",
    "# t1w_landmarks = (hiseq_t1npyfile if os.path.isfile(hiseq_t1npyfile) else \\\n",
    "#                  tio.HistogramStandardization.train(train_images20T1, output_path = hiseq_t1npyfile))\n",
    "# # #torch.save(t1w_landmarks, hiseq_t1npyfile)\n",
    "\n",
    "# hiseq_t1cnpyfile =  os.path.join(save_dir, f\"histeq_t1cw_{file_prefix}.npy\")\n",
    "# t1cw_landmarks = (hiseq_t1cnpyfile if os.path.isfile(hiseq_t1cnpyfile) else \\\n",
    "#                   tio.HistogramStandardization.train(train_images20T1ce, output_path = hiseq_t1cnpyfile))\n",
    "# #torch.save(t1cw_landmarks, hiseq_t1cnpyfile)\n",
    "\n",
    "\n",
    "# hiseq_t2npyfile = os.path.join(save_dir, f\"histeq_t2w_{file_prefix}.npy\")\n",
    "# t2w_landmarks = (hiseq_t2npyfile if os.path.isfile(hiseq_t2npyfile) else \\\n",
    "#                  tio.HistogramStandardization.train(train_images20T2, output_path = hiseq_t2npyfile))\n",
    "# #torch.save(t2w_landmarks, hiseq_t2npyfile)\n",
    "\n",
    "# hiseq_flairnpyfile = os.path.join(save_dir, f\"histeq_flair_{file_prefix}.npy\")\n",
    "# flair_landmarks = (hiseq_flairnpyfile if os.path.isfile(hiseq_flairnpyfile) else \\\n",
    "#                    tio.HistogramStandardization.train(train_images20Flair, output_path = hiseq_flairnpyfile))\n",
    "# #torch.save(flair_landmarks, hiseq_flairnpyfile)\n",
    "\n",
    "file_prefix = 'DynUNet_Brats20_3CV_4Chnls1PatchSWIRngr21_2nclass_1Patch'\n",
    "savedirname = 'DynUNetVariants_Brats20'\n",
    "save_dir = os.path.join('/raid/brats2021/pthBraTS2020_IDHGenomics', savedirname)\n",
    "if not os.path.exists(save_dir):\n",
    "    os.makedirs(save_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5899c8d7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "85141f2e",
   "metadata": {},
   "source": [
    "## Classes for Monai/Pytorch compose class"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85fa3429",
   "metadata": {},
   "source": [
    "A class to rearrange label mask array as \n",
    "- [0, :, :, :] = the multi class mask (class labels: 0 (background), 1, 2, and 4)\n",
    "- [1, :, :, :] = the whole tumor mask (class labels: 0 (background), and 1)\\\n",
    "Not using here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "be98e864",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvertToMultiChannelPlusWT(MapTransform):\n",
    "    \n",
    "    \"\"\"\n",
    "     GD-enhancing tumor (ET â€” label 4), \n",
    "     the peritumoral edema (ED â€” label 2), and \n",
    "     the necrotic and non-enhancing tumor core (NCR/NET â€” label 1)\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    def __call__(self, data):\n",
    "        d = dict(data)\n",
    "        for key in self.keys:\n",
    "            result = []\n",
    "            \n",
    "            d[key]=np.squeeze(d[key], axis = 0) # Converting 1, H, W, D to H, W, D\n",
    "            result.append(d[key])\n",
    "\n",
    "            # merge labels 1, 2 and 4 to construct WT\n",
    "            result.append(\n",
    "                np.logical_or(\n",
    "                    np.logical_or(d[key] == 2, d[key] == 4), d[key] == 1\n",
    "                )\n",
    "            )\n",
    "            ## merge label 1 and label 4 to construct TC\n",
    "            #result.append(np.logical_or(d[key] == 1, d[key] == 4))\n",
    "            ## label 4 is ET\n",
    "            #result.append(d[key] == 4)\n",
    "            d[key] = np.stack(result, axis=0).astype(np.uint8)\n",
    "        return d"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f9a7ad8",
   "metadata": {},
   "source": [
    "#### Define a new transform to convert brain tumor labels\n",
    "Here we convert the multi-classes labels into multi-labels segmentation task in One-Hot format.\\\n",
    "Not using here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "69bde161",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvertToMultiChannelBasedOnBratsClassesd(MapTransform):\n",
    "    \n",
    "    \"\"\"\n",
    "     GD-enhancing tumor (ET â€” label 4), \n",
    "     the peritumoral edema (ED â€” label 2), and \n",
    "     the necrotic and non-enhancing tumor core (NCR/NET â€” label 1)\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    def __call__(self, data):\n",
    "        d = dict(data)\n",
    "        for key in self.keys:\n",
    "            result = []\n",
    "            \n",
    "            d[key]=np.squeeze(d[key], axis = 0) # Converting 1, H, W, D to H, W, D\n",
    "\n",
    "            # merge labels 1, 2 and 4 to construct WT\n",
    "            result.append(\n",
    "                np.logical_or(\n",
    "                    np.logical_or(d[key] == 2, d[key] == 4), d[key] == 1\n",
    "                )\n",
    "            )\n",
    "            # merge label 1 and label 4 to construct TC\n",
    "            result.append(np.logical_or(d[key] == 1, d[key] == 4))\n",
    "            # label 4 is ET\n",
    "            result.append(d[key] == 4)\n",
    "            d[key] = np.stack(result, axis=0).astype(np.float32)\n",
    "        return d"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76ac8846",
   "metadata": {},
   "source": [
    "#### A class to add new key having the tumor mask (GT) to the existing data dictionary\n",
    "The new key, ***label_mask*** will have the same dimension (size: 4,x,x,x) with image array (size: 4,x,x,x)\\\n",
    "Using in ***compose*** class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "16586fc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvertToIDHLabel2WTd(MapTransform):\n",
    "    \n",
    "    \"\"\"\n",
    "     GD-enhancing tumor (ET â€” label 4), \n",
    "     the peritumoral edema (ED â€” label 2), and \n",
    "     the necrotic and non-enhancing tumor core (NCR/NET â€” label 1)\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, keys: KeysCollection, IDH_label_key:str = 'IDH_label') -> None:\n",
    "\n",
    "        super().__init__(keys)\n",
    "        self.IDH_label_key = IDH_label_key\n",
    "       \n",
    "    def __call__(self, data):\n",
    "        d = dict(data)\n",
    "        for key in self.keys:\n",
    "            # merge labels 1, 2 and 4 to construct WT\n",
    "            #WT = np.logical_or(np.logical_or(d[key] == 2, d[key] == 4), d[key] == 1).astype(np.uint8)\n",
    "            result = []\n",
    "            WT = np.squeeze(d[key], axis = 0)\n",
    "            if d[self.IDH_label_key].item() == 1:\n",
    "                WT=np.multiply(WT, 2)\n",
    "                #WT = 2*WT\n",
    "            \n",
    "            result.append(WT==1)\n",
    "            result.append(WT==2)\n",
    "            \n",
    "            d[key] = np.stack(result, axis = 0).astype(np.float32)\n",
    "            \n",
    "    \n",
    "        return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1290bb84",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Convert2WTd(MapTransform):\n",
    "    \n",
    "    \"\"\"\n",
    "     GD-enhancing tumor (ET â€” label 4), \n",
    "     the peritumoral edema (ED â€” label 2), and \n",
    "     the necrotic and non-enhancing tumor core (NCR/NET â€” label 1)\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    def __call__(self, data):\n",
    "        d = dict(data)\n",
    "        for key in self.keys:\n",
    "            result = []\n",
    "            \n",
    "            # merge labels 1, 2 and 4 to construct WT\n",
    "            WT = np.logical_or(np.logical_or(d[key] == 2, d[key] == 4), d[key] == 1).astype(np.uint8)\n",
    "\n",
    "            d[f'{key}_mask'] = WT\n",
    "            d[f'{key}'] = WT\n",
    "    \n",
    "        return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c1434cf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SpatialCropWTCOMd(MapTransform):\n",
    "    \n",
    "    \"\"\"\n",
    "     GD-enhancing tumor (ET â€” label 4), \n",
    "     the peritumoral edema (ED â€” label 2), and \n",
    "     the necrotic and non-enhancing tumor core (NCR/NET â€” label 1)\n",
    "\n",
    "    \"\"\"\n",
    "    def __init__(self, keys: KeysCollection, roi_size, COM_label_key:str = 'label_mask') -> None:\n",
    "\n",
    "        super().__init__(keys)\n",
    "        self.COM_label_key = COM_label_key\n",
    "        self.roi_size = roi_size\n",
    "    \n",
    "    def __call__(self, data):\n",
    "        d = dict(data)\n",
    "        for key in self.keys:\n",
    "            result = []\n",
    "            Coms = np.array([ndimage.measurements.center_of_mass(lbl) for lbl in list(d[self.COM_label_key])])\n",
    "            Coms[np.isnan(Coms)] = 70\n",
    "            Coms=Coms[0].astype(np.uint16).tolist()\n",
    "        \n",
    "            sc_com= SpatialCrop(roi_center= Coms, roi_size=self.roi_size)\n",
    "            d[key] = sc_com(d[key])\n",
    "                \n",
    "        return d"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5294370c",
   "metadata": {},
   "source": [
    "### A class for blending (alpha or other types) 4 channel image array with label(GT) if necessay\n",
    "Not using here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fb8e8d10",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AddBlendImaged(MapTransform):\n",
    "    \"\"\"\n",
    "          we do not need labels as it is a generative problem\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    \n",
    "    def __init__(self, keys: KeysCollection, label_key:str = 'label') -> None:\n",
    "        \n",
    "        super().__init__(keys)\n",
    "        self.label_key = label_key\n",
    "    \n",
    "    \n",
    "    def __call__(self, data):\n",
    "     \n",
    "        d = dict(data)\n",
    "        kyesImgLbl = self.keys\n",
    "        \n",
    "        for key in self.keys:\n",
    "            num_labels = d[self.label_key].shape[0]\n",
    "            class_weights = [0.04, 0.15, 0.5, .2]\n",
    "            \n",
    "            dd=np.zeros_like(d[self.label_key][0:1,...])\n",
    "            \n",
    "            #for i in range(num_labels):\n",
    "            \n",
    "            values, counts = np.unique(d[self.label_key], return_counts=True)\n",
    "            dlbl = d[self.label_key]\n",
    "            \n",
    "            for cl in range(1,  len(values)):\n",
    "                \n",
    "                d[self.label_key] = np.where(d[self.label_key]==values[cl], class_weights[cl], d[self.label_key])\n",
    "                \n",
    "                #dd = dd + label_weights[i]*d[self.label_key][i:i+1,:,:,:]\n",
    "            \n",
    "            #blend_d = 0.33*d[key][1:2,:,:,:] + 0.33*d[key][2:3,:,:,:] + 0.34 * dd\n",
    "            \n",
    "            blend_d = 0.7*d[key][0:1,:,:,:] + 0.3 * d[self.label_key]\n",
    "            d[key] = np.concatenate((d[key], blend_d),axis=0)\n",
    "            \n",
    "        return d"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c8903ad",
   "metadata": {},
   "source": [
    "### A class for concatenating brain_mask and label(GT) with 4-channel image array if necessary\n",
    "(Not using here)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "91f1bd8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConcatLabelBrainmaskd(MapTransform):\n",
    "    \"\"\"\n",
    "          we do not need labels as it is a generative problem\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, keys: KeysCollection, image_key = 'image', label_key = 'label', \n",
    "                 brain_mask_key = 'brain_mask') -> None:\n",
    "        \n",
    "        super().__init__(keys)\n",
    "        self.brain_mask_key = brain_mask_key\n",
    "        self.image_key = image_key\n",
    "        self.label_key = label_key\n",
    "    \n",
    "    \n",
    "    def __call__(self, data):\n",
    "     \n",
    "        d = dict(data)\n",
    "        #d[self.image_key] = np.concatenate((d[self.image_key], d[self.label_key], d[self.brain_mask_key][0:1]), axis = 0)\n",
    "        d[self.image_key] = np.concatenate((d[self.image_key], d[self.label_key][0:1]), axis = 0)\n",
    "        \n",
    "        return d"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48521d4e",
   "metadata": {},
   "source": [
    "### Implementing channelwise histogram normalization\n",
    "(Not using here)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "099af4fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HistogramNormalizeChannelWised(MapTransform):\n",
    "    \"\"\"\n",
    "          we do not need labels as it is a generative problem\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, keys: KeysCollection, brain_mask_key = 'brain_mask', min=0, max=255) -> None:\n",
    "        \n",
    "        super().__init__(keys)\n",
    "        self.brain_mask_key = brain_mask_key\n",
    "        self.histnorms = HistogramNormalize(num_bins=256, min=min, max=max)\n",
    "    \n",
    "    \n",
    "    def __call__(self, data):\n",
    "     \n",
    "        d = dict(data)\n",
    "        for key in self.keys:\n",
    "            nchnl = d[key].shape[0]\n",
    "            for ch in range(nchnl):\n",
    "                d[key][ch] = self.histnorms(d[key][ch], d[self.brain_mask_key][ch])\n",
    "        \n",
    "        return d"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0b118ca",
   "metadata": {},
   "source": [
    "### Defining traning and validation transforms\n",
    "\n",
    "- Training transform includes:\n",
    "    - LoadImaged\n",
    "    - EnsureChannelFirstd\n",
    "    - HistogramNormalizeChannelWised: Histogram normalization channel wise (custom class defined aboove)\n",
    "    - NormalizeIntensityd\n",
    "    - RandRotate90d\n",
    "    - RandZoomd\n",
    "    - ConvertToIDHLabel2WTd (custom class defined above)\n",
    "    - CropForegroundd: Cropping foreground based on the whole tumor mask (WT GT)\n",
    "    - RandCropByPosNegLabeld: Randomly cropping 8 patches based on 3: 1 (WT : non tumor tissus) ratio\n",
    "    - RandGaussianNoised\n",
    "    - RandStdShiftIntensityd\n",
    "    - RandFlipd\n",
    "    \n",
    "- validation transform includes:\n",
    "    - LoadImaged\n",
    "    - EnsureChannelFirstd\n",
    "    - HistogramNormalizeChannelWised: Histogram normalization channel wise (custom class defined aboove)\n",
    "    - NormalizeIntensityd\n",
    "    - ConvertToIDHLabel2WTd (custom class defined above)\n",
    "    \n",
    "Most of transfroms are implemented using [Monai](https://docs.monai.io/en/latest/transforms.html#dictionary-transforms) library\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2a1afb1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def threshold_foreground(x):\n",
    "    # threshold at not equal to 0\n",
    "    return x == 1\n",
    "\n",
    "\n",
    "#Resized(keys=keys[0:-1], spatial_size=patch_size, mode = ('area','nearest','nearest')),\n",
    "\n",
    "# ConvertToMultiChannelBasedOnBratsClassesd(keys = ['label']),\n",
    "# ConcatLabelBrainmaskd(keys = None, image_key = 'image', label_key = 'label', brain_mask_key = 'brain_mask'),\n",
    "# CropForegroundd(keys=keys[0:-1], source_key=\"brain_mask\", select_fn = threshold_foreground, start_coord_key='fg_start_coord', end_coord_key='fg_end_coord'),\n",
    "# SpatialPadd(keys=keys[0:-1], spatial_size=patch_size),\n",
    "\n",
    "# RandGaussianSmoothd(\n",
    "#     keys=[\"image\"],\n",
    "#     sigma_x=(0.5, 1.15),\n",
    "#     sigma_y=(0.5, 1.15),\n",
    "#     sigma_z=(0.5, 1.15),\n",
    "#     prob=0.3,\n",
    "# ),\n",
    "\n",
    "# RandScaleIntensityd(keys=[\"image\"], factors=0.3, prob=0.3),\n",
    "# RandGibbsNoised(keys=[\"image\"], prob=0.3, alpha=(0.1, 0.5), as_tensor_output=False),\n",
    "\n",
    "          \n",
    "# DataStatsd(keys=keys[0:-1], prefix=\"Data\", data_type=True, data_shape=True, value_range=True, data_value=False),\n",
    "#DataStatsd(keys=keysExt[0:-1], prefix=\"Data\", data_type=True, data_shape=True, value_range=True, data_value=False),\n",
    "\n",
    "\n",
    "def get_task_transforms(patch_size, task='train', pos_sample_num=1, neg_sample_num=1, num_samples=1):\n",
    "    \n",
    "    #spatial_size=(30, 30, 30)\n",
    "    orig_img_size = (240, 240, 155)\n",
    "\n",
    "    if task=='train':\n",
    "        keys = [\"image\", 'label', 'brain_mask', 'IDH_label']\n",
    "        keysExt = [\"image\", 'label', 'brain_mask', 'label_mask', 'IDH_label']\n",
    "        \n",
    "        all_transform = [\n",
    "            \n",
    "            LoadImaged(keys=keys[0:-1], reader = \"NibabelReader\"),\n",
    "            EnsureChannelFirstd(keys=keys[0:-1]),\n",
    "            #adapter_tioChannelWise2monai(tiofn = tio.HistogramStandardization, mode = 'train',landmarks = landmarks_dict),\n",
    "            #HistogramNormalizeChannelWised(keys = ['image'], brain_mask_key = 'brain_mask', min = 1, max = 65535),\n",
    "            \n",
    "            RandAffined(\n",
    "                keys = keys[0:-1],\n",
    "                prob=0.2,\n",
    "                spatial_size= orig_img_size, #(240, 240, 155),\n",
    "                rotate_range=np.pi/9,\n",
    "                scale_range=(0.1, 0.1),\n",
    "                mode=(\"bilinear\", \"nearest\", \"nearest\"),\n",
    "                as_tensor_output=False,\n",
    "                padding_mode = (\"zeros\", \"zeros\", \"zeros\"),\n",
    "            ),\n",
    "            \n",
    "            RandRotate90d(keys=keys[0:-1], prob=0.3, spatial_axes=[0, 2]),\n",
    "\n",
    "            RandZoomd(\n",
    "                keys=keys[0:-1],\n",
    "                min_zoom=0.9,\n",
    "                max_zoom=1.1,\n",
    "                mode=(\"trilinear\", \"nearest\", \"nearest\"),\n",
    "                align_corners=(True, None, None),\n",
    "                prob=0.3,\n",
    "            ),\n",
    "            \n",
    "            #ConvertToIDHLabel2WTd(keys = [\"label\"]),\n",
    "            Convert2WTd(keys = [\"label\"]),\n",
    "            ConvertToIDHLabel2WTd(keys = [\"label\"], IDH_label_key = 'IDH_label'),\n",
    "            CropForegroundd(keys=keysExt[0:-1], source_key=\"brain_mask\", select_fn = threshold_foreground, start_coord_key='fg_start_coord', end_coord_key='fg_end_coord'),\n",
    "            Spacingd(keys = keysExt[0:-1], pixdim=(1.25, 1.25, 1.25), mode = ('bilinear','nearest', 'nearest', 'nearest')),\n",
    "            NormalizeIntensityd(keys=[\"image\"], nonzero=True, channel_wise=True),\n",
    "            \n",
    "            ResizeWithPadOrCropd(keys = keysExt[0:-1], spatial_size = (128, 160, 128)),\n",
    "            RandGaussianNoised(keys=[\"image\"], std=0.01, prob=0.3),\n",
    "            RandStdShiftIntensityd(keys = [\"image\"], factors=0.3, nonzero=True, channel_wise=True, prob=0.3), \n",
    "            RandFlipd(keys=keysExt[0:-1], prob=0.5, spatial_axis=0),\n",
    "            RandFlipd(keys=keysExt[0:-1], prob=0.5, spatial_axis=1),\n",
    "            RandFlipd(keys=keysExt[0:-1], prob=0.5, spatial_axis=2),\n",
    "            \n",
    "            #CropForegroundd(keys=keysExt[0:-1], source_key=\"label_mask\", select_fn = threshold_foreground, start_coord_key='fg_start_coord', end_coord_key='fg_end_coord'),\n",
    "            #ResizeWithPadOrCropd(keys = keysExt[0:-1], spatial_size = patch_size),\n",
    "#             RandCropByLabelClassesd(\n",
    "#                 keys=keysExt[0:-1],            \n",
    "#                 label_key = \"label_mask\",\n",
    "#                 spatial_size = patch_size,    \n",
    "#                 ratios= [1, 10],     \n",
    "#                 num_classes=2,              \n",
    "#                 num_samples=1,\n",
    "#                 image_key=\"brain_mask\",\n",
    "#                 image_threshold=0,\n",
    "#             ),\n",
    "            \n",
    "            SpatialCropWTCOMd(keys=keysExt[0:-1], roi_size=patch_size, COM_label_key = \"label_mask\"),\n",
    "            SpatialPadd(keys = keysExt[0:-1], spatial_size = patch_size),\n",
    "        \n",
    "            CastToTyped(keys=keysExt, dtype=(np.float32, np.uint8, np.uint8, np.uint8, np.float32)),\n",
    "            ToTensord(keys=keysExt),\n",
    "\n",
    "#             #EnsureTyped(keys=keys, data_type = \"tensor\"),\n",
    "#             #ToDeviced(keys = keys, device = deviceName),\n",
    "        ]\n",
    "        \n",
    "        \n",
    "        \n",
    "    elif task=='validation':\n",
    "    \n",
    "        keys = [\"image\", 'label', 'brain_mask', 'IDH_label']\n",
    "        keysExt = [\"image\", 'label', 'brain_mask', 'label_mask', 'IDH_label']\n",
    "        all_transform = [\n",
    "            \n",
    "            LoadImaged(keys=keys[0:-1], reader = \"NibabelReader\"),\n",
    "            EnsureChannelFirstd(keys=keys[0:-1]),\n",
    "            #adapter_tioChannelWise2monai(tiofn = tio.HistogramStandardization, mode = 'train',landmarks = landmarks_dict),\n",
    "            #HistogramNormalizeChannelWised(keys = ['image'], brain_mask_key = 'brain_mask', min = 1, max = 65535),\n",
    "            #ConvertToIDHLabel2WTd(keys = [\"label\"]),\n",
    "            Convert2WTd(keys = [\"label\"]),\n",
    "            ConvertToIDHLabel2WTd(keys = [\"label\"], IDH_label_key = 'IDH_label'),\n",
    "            CropForegroundd(keys=keysExt[0:-1], source_key=\"brain_mask\", select_fn = threshold_foreground, start_coord_key='fg_start_coord', end_coord_key='fg_end_coord'),\n",
    "            Spacingd(keys = keysExt[0:-1], pixdim=(1.25, 1.25, 1.25), mode = ('bilinear','nearest', 'nearest', 'nearest')),\n",
    "            NormalizeIntensityd(keys=[\"image\"], nonzero=True, channel_wise=True),\n",
    "            ResizeWithPadOrCropd(keys = keysExt[0:-1], spatial_size = (128, 160, 128)),\n",
    "            #CropForegroundd(keys=keysExt[0:-1], source_key=\"label_mask\", select_fn = threshold_foreground, start_coord_key='fg_start_coord', end_coord_key='fg_end_coord'),\n",
    "            SpatialCropWTCOMd(keys=keysExt[0:-1], roi_size=patch_size, COM_label_key = \"label_mask\"),\n",
    "            SpatialPadd(keys = keysExt[0:-1], spatial_size = patch_size),\n",
    "#             RandCropByPosNegLabeld(\n",
    "#                 keys=keysExt[0:-1],\n",
    "#                 label_key=\"label_mask\",\n",
    "#                 spatial_size=patch_size,\n",
    "#                 pos=pos_sample_num,\n",
    "#                 neg=neg_sample_num,\n",
    "#                 num_samples=num_samples,\n",
    "#                 image_key=\"brain_mask\",\n",
    "#                 image_threshold=0,\n",
    "#             ),\n",
    "\n",
    "            CastToTyped(keys=keysExt, dtype=(np.float32, np.uint8, np.uint8, np.uint8, np.float32)),\n",
    "            ToTensord(keys=keysExt),\n",
    "            #EnsureTyped(keys=keys, data_type = \"tensor\"),\n",
    "            #ToDeviced(keys = keys, device = deviceName),\n",
    "        ]\n",
    "        \n",
    "    else:\n",
    "        print('print task either train or validation here')\n",
    "\n",
    "\n",
    "    return Compose(all_transform)\n",
    "\n",
    "# def create_cachedir(cache_dir):\n",
    "#     if not os.path.exists(cache_dir):\n",
    "#         os.makedirs(cache_dir)\n",
    "#     return 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59d6539e",
   "metadata": {},
   "source": [
    "### Section for visual inspection and debugging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "05f4f1c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20, 12)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#patch_size=(128, 160, 128)\n",
    "patch_size=(64, 80, 64)\n",
    "train_transforms = get_task_transforms(patch_size, task='train', pos_sample_num=1, neg_sample_num=1, num_samples=1)\n",
    "val_transforms = get_task_transforms(patch_size, task='validation', pos_sample_num=1, neg_sample_num=1, num_samples=1)\n",
    "len(train_transforms), len(val_transforms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "475ef436",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "72dbf3d6",
   "metadata": {},
   "source": [
    "all_train_dataset = Dataset(data=train_files[0:10], transform=val_transforms)\n",
    "print(all_train_dataset[0]['image'].shape)\n",
    "for kj in all_train_dataset:\n",
    "    #asub = kj\n",
    "    #num_samples=12\n",
    "    #pindx = random.choice(np.arange(12, dtype = np.uint8))\n",
    "    #kj = kj[pindx]\n",
    "    print('IDH_label:', kj['IDH_label'])\n",
    "    print(kj['label'].shape, kj['label'].unique(return_counts = True))\n",
    "    print(kj['label_mask'].shape, kj['label_mask'].unique(return_counts = True))\n",
    "    unque = kj['label_mask'].unique(return_counts = True)[1]\n",
    "    print('\\n Background ratio:', unque[0]/unque.sum(), ' Tumor(=1): ',  unque[1]/unque.sum())\n",
    "    print('#'*100)\n",
    "#print(asub['image'].shape)\n",
    "# print(asub['image'][0].min(),asub['image'][0].max(), asub['image'][0].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98ffd368",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "078a6c21",
   "metadata": {},
   "outputs": [],
   "source": [
    "#few_train_dataset = Dataset(data=train_files[0:10], transform=train_transforms)\n",
    "# asub = few_train_dataset[5] \n",
    "# view(image = asub['image'][3].cpu(), label_image = asub['label_mask'][0].cpu())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c18ead8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5287bfae",
   "metadata": {},
   "source": [
    "### Few investigation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4872eb1d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([1.        , 0.73076923]), array([38, 52]))"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "afold_train_dataset = monai.data.Dataset(data=train_folds['fold0'], transform=train_transforms)\n",
    "#train_folds['fold0_IDH_label']\n",
    "uval, ucnt = np.unique(train_folds['fold0_IDH_label'], return_counts=True)\n",
    "weight = 1./(ucnt/ucnt.min())\n",
    "#weight = np.array([0.55, 0.45])\n",
    "sample_weights = np.array([weight[int(t)] for t in train_folds['fold0_IDH_label']])\n",
    "sample_weights = torch.from_numpy(sample_weights)\n",
    "weight, ucnt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c2503e2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c3b36b4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "703ed9de",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "db47abc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CrossEntropyInstWLogitLoss(nn.Module):\n",
    "    def __init__(self, is_smooth=False, label_smoothing = 0.1):\n",
    "        super().__init__()\n",
    "        self.is_smooth = is_smooth\n",
    "        self.label_smoothing = label_smoothing\n",
    "        \n",
    "       \n",
    "    def forward(self, y_pred, y_true):\n",
    "\n",
    "        if self.is_smooth == True:\n",
    "            y_true = y_true.float() * (1 - self.label_smoothing) + 0.5 * self.label_smoothing\n",
    "\n",
    "        y_true=y_true.type_as(y_pred)   ### y_pred, and y_true should be same size and same data type\n",
    "        \n",
    "        \n",
    "        #deviceidx = y_pred.get_device()\n",
    "        #device = torch.device('cpu') if deviceidx == -1 else torch.device(f'cuda:{deviceidx}')\n",
    "        #loss = F.binary_cross_entropy_with_logits(y_pred.to(device), y_true.to(device), pos_weight = weight.to(device))  ##pos_weight = weight \n",
    "        loss = F.binary_cross_entropy_with_logits(y_pred, y_true) \n",
    "        return loss\n",
    "\n",
    "\n",
    "# class DeepDiceCELogitInstLoss(nn.Module):\n",
    "#     def __init__(self):\n",
    "#         super().__init__()\n",
    "#         #self.volweight = torch.softmax(torch.tensor([0.12, 0.33, 0.55]), dim = 0)\n",
    "#         self.dice = DiceLoss(include_background=False, to_onehot_y=True, softmax=True, squared_pred=True, batch = False)  # reduction = \"none\", batch = True\n",
    "#         #self.smcross_entropy = CrossEntropyInstLoss()  ### was none torch.Tensor([0.66, 0.33, 1]), torch.tensor(self.volweight)\n",
    "\n",
    "#     def forward(self, y_pred, y_true):\n",
    "        \n",
    "#         y_true = y_true.unsqueeze(dim=0).expand(y_pred.shape[1],-1,-1,-1,-1, -1)\n",
    "#         #return sum([0.5 ** i * ((self.dice(p, l)) + self.smcross_entropy(p, l)) \\\n",
    "#         #            for i, (p, l) in enumerate(zip(torch.unbind(y_pred, dim=1), torch.unbind(y_true, dim=0)))])\n",
    "#         return sum([0.5 ** i * self.dice(p, l) for i, (p, l) in enumerate(zip(torch.unbind(y_pred, dim=1), torch.unbind(y_true, dim=0)))])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class DeepDiceCELogitInstLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        #self.volweight = torch.softmax(torch.tensor([0.12, 0.33, 0.55]), dim = 0)\n",
    "        self.dice = DiceLoss(to_onehot_y=False, sigmoid=True, squared_pred=True, batch = True)  # reduction = \"none\", False\n",
    "        self.logitcross_entropy = CrossEntropyInstWLogitLoss()  ### was none torch.Tensor([0.66, 0.33, 1]), torch.tensor(self.volweight)\n",
    "\n",
    "    def forward(self, y_pred, y_true):\n",
    "        \n",
    "        y_true = y_true.unsqueeze(dim=0).expand(y_pred.shape[1],-1,-1,-1,-1, -1)\n",
    "        return sum([0.5 ** i * ((self.dice(p, l)) + self.logitcross_entropy(p, l)) \\\n",
    "                    for i, (p, l) in enumerate(zip(torch.unbind(y_pred, dim=1), torch.unbind(y_true, dim=0)))])\n",
    "    \n",
    "loss_function = DeepDiceCELogitInstLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "17910ea7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2.6450)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xxp = torch.randint(0,4,size=(6,3, 128, 128, 128))\n",
    "#pred = [torch.randn(6,3,8,8,6), torch.randn(6,3,8,8,6), torch.randn(6,3,8,8,6)]\n",
    "pred = torch.stack([torch.randn(6, 3, 128, 128, 128), torch.randn(6, 3, 128, 128, 128), torch.randn(6, 3, 128, 128, 128), torch.randn(6, 3, 128, 128, 128)], dim=1)\n",
    "loss_function(pred, xxp.float())\n",
    "#loss_function(pred, xxp.float())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "20d65221",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([38, 52])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ucnt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "41d3a536",
   "metadata": {},
   "outputs": [],
   "source": [
    "#afold_train_dataset[200]['IDH_label'], afold_train_dataset[200]['label'].unique(return_counts = True)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "3489593f",
   "metadata": {},
   "source": [
    "#sampler = WeightedRandomSampler(sample_weights, num_samples= len(samples_weight))\n",
    "sampler = WeightedRandomSampler(sample_weights, num_samples= len(sample_weights), replacement=True)\n",
    "\n",
    "afold_train_loader = monai.data.DataLoader(afold_train_dataset, batch_size=32, sampler=sampler)\n",
    "#afold_train_loader = torch.utils.data.DataLoader(afold_train_dataset, batch_size=32, sampler=sampler)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "11bf0ff4",
   "metadata": {},
   "source": [
    "for ibatch in afold_train_loader:\n",
    "    ibatch_IDH = ibatch['IDH_label']\n",
    "    print(torch.eq(ibatch_IDH, 0).sum(), torch.eq(ibatch_IDH, 1).sum())\n",
    "    print('#'*50)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "367ddc72",
   "metadata": {},
   "source": [
    "numDataPoints = 1000\n",
    "data_dim = 5\n",
    "bs = 100\n",
    "\n",
    "# Create dummy data with class imbalance 9 to 1\n",
    "data = torch.FloatTensor(numDataPoints, data_dim)\n",
    "target = np.hstack((np.zeros(int(numDataPoints * 0.9), dtype=np.int32),\n",
    "                    np.ones(int(numDataPoints * 0.1), dtype=np.int32)))\n",
    "\n",
    "print('target train 0/1: {}/{}'.format(len(np.where(target == 0)[0]), len(np.where(target == 1)[0])))\n",
    "\n",
    "class_sample_count = np.array(\n",
    "    [len(np.where(target == t)[0]) for t in np.unique(target)])\n",
    "weight = 1. / class_sample_count\n",
    "samples_weight = np.array([weight[t] for t in target])\n",
    "samples_weight"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91dbd62c",
   "metadata": {},
   "source": [
    "## Custom editing of SegResNetVAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "793d67e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3]] \n",
      " [[1, 1, 1], [2, 2, 2], [2, 2, 2], [2, 2, 2], [2, 2, 2]]\n",
      "strides length 5\n"
     ]
    }
   ],
   "source": [
    "def get_kernels_strides(sizes, spacings):\n",
    "    #sizes, spacings = patch_size[task_id], spacing[task_id]\n",
    "    strides, kernels = [], []\n",
    "\n",
    "    while True:\n",
    "        spacing_ratio = [sp / min(spacings) for sp in spacings]\n",
    "        stride = [\n",
    "            2 if ratio <= 2 and size >= 8 else 1\n",
    "            for (ratio, size) in zip(spacing_ratio, sizes)\n",
    "        ]\n",
    "        kernel = [3 if ratio <= 2 else 1 for ratio in spacing_ratio]\n",
    "        if all(s == 1 for s in stride):\n",
    "            break\n",
    "        sizes = [i / j for i, j in zip(sizes, stride)]\n",
    "        spacings = [i * j for i, j in zip(spacings, stride)]\n",
    "        kernels.append(kernel)\n",
    "        strides.append(stride)\n",
    "    strides.insert(0, len(spacings) * [1])\n",
    "    kernels.append(len(spacings) * [3])\n",
    "    return kernels, strides\n",
    "#task_id = \"01\"\n",
    "kernels, strides = get_kernels_strides(patch_size, spacing)\n",
    "#kernels.append([3, 3, 3])\n",
    "#strides.append([2, 2, 2])\n",
    "\n",
    "print(kernels,'\\n', strides)\n",
    "\n",
    "print('strides length', len(strides))\n",
    "#filters = [64, 96, 128, 192, 256, 384, 512, 768, 1024][: len(strides)]\n",
    "#filters = [16, 32, 64, 128, 160, 160][: len(strides)]\n",
    "#print(\"Filters:\", filters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "bfcf303b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# kernels, strides = get_kernels_strides((128, 128, 128), spacing)\n",
    "# #kernels, strides\n",
    "# kernels = [[3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 6, 3]]\n",
    "# strides = [[1, 1, 1], [2, 2, 2], [2, 2, 2], [2, 2, 2], [2, 2, 2], [2, 2, 2]]"
   ]
  },
  {
   "cell_type": "raw",
   "id": "a375f9b3",
   "metadata": {},
   "source": [
    "model = DynUNet(    \n",
    "    spatial_dims=3,\n",
    "    in_channels=4,\n",
    "    out_channels=3,\n",
    "    kernel_size=kernels,\n",
    "    strides=strides,\n",
    "    upsample_kernel_size=strides[1:],\n",
    "    norm_name=\"batch\",\n",
    "    deep_supervision=True,\n",
    "    deep_supr_num=2,\n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "f1e8a5cb",
   "metadata": {},
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "7eb08f3c",
   "metadata": {},
   "source": [
    "inps = torch.randn(3, 4, 64, 80, 64).to(device)\n",
    "x = model(inps)\n",
    "# # model\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99a81b2d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "id": "ea314309",
   "metadata": {},
   "source": [
    "from torchsummary import summary\n",
    "summary(model, (4, 64, 80, 64))\n",
    "# # inps = torch.randn(3, 4, 48, 64, 48)\n",
    "# # litConv = nn.Conv3d(4, 64, kernel_size=(7, 7, 7), stride=(2, 2, 2), padding=(3, 3, 3), bias=False)\n",
    "# # litConv(inps).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78d05967",
   "metadata": {},
   "source": [
    "### Defining loss functions\n",
    "- ***CrossEntropyLogitLoss*** Cross entropy logit loss from [PyTorch](https://pytorch.org/docs/stable/generated/torch.nn.BCEWithLogitsLoss.html)\n",
    "- ***DiceCELoss*** Dice + Cross entropy loss from Monai\n",
    "https://docs.monai.io/en/latest/losses.html\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "957a0a00",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CrossEntropyInstWLogitLoss(nn.Module):\n",
    "    def __init__(self, is_smooth=False, label_smoothing = 0.1):\n",
    "        super().__init__()\n",
    "        self.is_smooth = is_smooth\n",
    "        self.label_smoothing = label_smoothing\n",
    "        \n",
    "       \n",
    "    def forward(self, y_pred, y_true):\n",
    "\n",
    "        if self.is_smooth == True:\n",
    "            y_true = y_true.float() * (1 - self.label_smoothing) + 0.5 * self.label_smoothing\n",
    "\n",
    "        y_true=y_true.type_as(y_pred)   ### y_pred, and y_true should be same size and same data type\n",
    "        \n",
    "        \n",
    "        #deviceidx = y_pred.get_device()\n",
    "        #device = torch.device('cpu') if deviceidx == -1 else torch.device(f'cuda:{deviceidx}')\n",
    "        #loss = F.binary_cross_entropy_with_logits(y_pred.to(device), y_true.to(device), pos_weight = weight.to(device))  ##pos_weight = weight \n",
    "        loss = F.binary_cross_entropy_with_logits(y_pred, y_true) \n",
    "        return loss\n",
    "\n",
    "\n",
    "# class DeepDiceCELogitInstLoss(nn.Module):\n",
    "#     def __init__(self):\n",
    "#         super().__init__()\n",
    "#         #self.volweight = torch.softmax(torch.tensor([0.12, 0.33, 0.55]), dim = 0)\n",
    "#         self.dice = DiceLoss(include_background=False, to_onehot_y=True, softmax=True, squared_pred=True, batch = False)  # reduction = \"none\", batch = True\n",
    "#         #self.smcross_entropy = CrossEntropyInstLoss()  ### was none torch.Tensor([0.66, 0.33, 1]), torch.tensor(self.volweight)\n",
    "\n",
    "#     def forward(self, y_pred, y_true):\n",
    "        \n",
    "#         y_true = y_true.unsqueeze(dim=0).expand(y_pred.shape[1],-1,-1,-1,-1, -1)\n",
    "#         #return sum([0.5 ** i * ((self.dice(p, l)) + self.smcross_entropy(p, l)) \\\n",
    "#         #            for i, (p, l) in enumerate(zip(torch.unbind(y_pred, dim=1), torch.unbind(y_true, dim=0)))])\n",
    "#         return sum([0.5 ** i * self.dice(p, l) for i, (p, l) in enumerate(zip(torch.unbind(y_pred, dim=1), torch.unbind(y_true, dim=0)))])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class DeepDiceCELogitInstLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        #self.volweight = torch.softmax(torch.tensor([0.12, 0.33, 0.55]), dim = 0)\n",
    "        self.dice = DiceLoss(to_onehot_y=False, sigmoid=True, squared_pred=True, batch = True)  # reduction = \"none\", False\n",
    "        self.logitcross_entropy = CrossEntropyInstWLogitLoss()  ### was none torch.Tensor([0.66, 0.33, 1]), torch.tensor(self.volweight)\n",
    "\n",
    "    def forward(self, y_pred, y_true):\n",
    "        \n",
    "        y_true = y_true.unsqueeze(dim=0).expand(y_pred.shape[1],-1,-1,-1,-1, -1)\n",
    "        return sum([0.5 ** i * ((self.dice(p, l)) + self.logitcross_entropy(p, l)) \\\n",
    "                    for i, (p, l) in enumerate(zip(torch.unbind(y_pred, dim=1), torch.unbind(y_true, dim=0)))])\n",
    "    \n",
    "loss_function = DeepDiceCELogitInstLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "62c6bd03",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2.6448)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xxp = torch.randint(0,4,size=(6,3, 128, 128, 128))\n",
    "#pred = [torch.randn(6,3,8,8,6), torch.randn(6,3,8,8,6), torch.randn(6,3,8,8,6)]\n",
    "pred = torch.stack([torch.randn(6, 3, 128, 128, 128), torch.randn(6, 3, 128, 128, 128), torch.randn(6, 3, 128, 128, 128), torch.randn(6, 3, 128, 128, 128)], dim=1)\n",
    "loss_function(pred, xxp.float())\n",
    "#loss_function(pred, xxp.float())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbdda614",
   "metadata": {},
   "source": [
    "#### A function to create ***cache_dir*** to save transformed outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "634f7587",
   "metadata": {},
   "outputs": [],
   "source": [
    "def removeAndcreate_cachedir(cache_dir):\n",
    "    if not os.path.exists(cache_dir):\n",
    "        os.makedirs(cache_dir)\n",
    "    else:\n",
    "        #print(\"Pass\")\n",
    "        shutil.rmtree(cache_dir)\n",
    "        os.makedirs(cache_dir)\n",
    "    return 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9549dc7",
   "metadata": {},
   "source": [
    "### Pytorch training loop\n",
    "\n",
    "Following functionalities are added\n",
    "\n",
    "- Implemeting learning rate finder\n",
    "- Defining Ranger21 optimizer (learning rate scheduler attached to it)\n",
    "- Implementing mixed precision (AMP)\n",
    "- Saving the model weights based on the performance on validation data\n",
    "- Executing 5 fold cross validation (CV) training/validation pipeline, saving a few best model's weights at each fold\n",
    "- Defining train_dataset/train_loader, and val_dataset/val_loader at each fold\n",
    "- Defining a CNN based classification model (DenseNet, EfficientNet, etc.) at each fold to make sure that all accumulated gradients get vanished\n",
    "\n",
    "The key variables which are used here\n",
    "- ***val_cache_dir:*** The path where the transformed ouputs of validaion files will be cached/saved\n",
    "- ***train_cache_dir:*** The path where the transformed ouputs of training files will be cached/saved\n",
    "- ***max_epochs:*** Total number of epochs\n",
    "- ***save_dir:*** The path to save the checkpoints/weights of the model\n",
    "- ***file_prefix:*** The text file where loss/accuracy is recoded like\n",
    "\n",
    "```\n",
    "current fold: 0 current epoch: 1, acc_metric: 0.4579 accuracy: 0.5085, f1score: 0.5085 epoch 1 average training loss: 0.7250 average validation loss: 0.7128 \n",
    "current fold: 0 current epoch: 2, acc_metric: 0.4876 accuracy: 0.5424, f1score: 0.5424 epoch 2 average training loss: 0.6961 average validation loss: 0.6940 \n",
    "current fold: 0 current epoch: 3, acc_metric: 0.4870 accuracy: 0.4915, f1score: 0.4915 epoch 3 average training loss: 0.6862 average validation loss: 0.6965 \n",
    "current fold: 0 current epoch: 4, acc_metric: 0.4882 accuracy: 0.5593, f1score: 0.5593 epoch 4 average training loss: 0.6715 average validation loss: 0.6885 \n",
    "current fold: 0 current epoch: 5, acc_metric: 0.5927 accuracy: 0.5593, f1score: 0.5593 epoch 5 average training loss: 0.6555 average validation loss: 0.6826 \n",
    "```\n",
    "\n",
    "- ***val_interval*** Epoch interval to investivate the model's performance on validation data. If the current performance is better than in previous epochs, the model's weights will be saved\n",
    "- ***key_metric_n_saved*** The number of model checkpoints we want to save. It it is set as 5, top 5 checkpoints/weights will be saved in 5 different ***.pth*** files  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "7e2d5a64",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#***Executed pipeline***\\\n",
    "#<img src=\"assets/ProposedIDHClass.png\" align=\"left\" width=\"1024\" height=\"1800\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "30ca03d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(train_files, train_files_IDH_label, val_files, val_files_IDH_label, batch_size = 2, epochs = 10, find_lr=False, cfold = 0):\n",
    "    \n",
    "\n",
    "    #     model = DenseNet201(spatial_dims=2, in_channels=3,\n",
    "    #                        out_channels=num_classes, pretrained=True).to(device)\n",
    "\n",
    "    # create spatial 3D\n",
    "    #model = MultiDenseNet(spatial_dims=3, in_channelsList=(4, 1, 1, 1, 1), out_channels=2, block_config = (6, 12, 24, 16)).to(device)\n",
    "    #model = monai.networks.nets.DenseNet121(spatial_dims=3, in_channels=4, out_channels=1).to(device)\n",
    "    #model = monai.networks.nets.DenseNet264(spatial_dims=3, in_channels=4, out_channels=1, init_features=64, growth_rate=32, block_config=(6, 12, 64, 48)).to(device)\n",
    "    #patch_size=(64, 80, 64)\n",
    "    num_classes = 2\n",
    "    \n",
    "    model = DynUNet(\n",
    "        spatial_dims=3,\n",
    "        in_channels=4,\n",
    "        out_channels=num_classes,\n",
    "        kernel_size=kernels,\n",
    "        strides=strides,\n",
    "        upsample_kernel_size=strides[1:],\n",
    "        norm_name=\"batch\",\n",
    "        #filters = filters,\n",
    "        deep_supervision=True,\n",
    "        res_block=False,\n",
    "        deep_supr_num=2,\n",
    "    ).to(device)\n",
    "    \n",
    "    \n",
    "    \n",
    "    auc_metric = ROCAUCMetric()\n",
    "    \n",
    "\n",
    "    #train_files, train_files_IDH_label, val_files, val_files_IDH_label = train_files[:48], train_files_IDH_label[:48], val_files[:16], val_files_IDH_label[:16]\n",
    "\n",
    "    \"\"\"\n",
    "    Block for using Monai's caching mechanishm for faster training\n",
    "    \"\"\"\n",
    "    \n",
    "    file_prefixfold = file_prefix  ##or file_prefix f\"{file_prefix}_fold{cfold}\" if saving cv file\n",
    "    data_rpath = '/home/mmiv-ml/data'\n",
    "    train_cache_dir = os.path.join(data_rpath,f'cachingDataset/{file_prefixfold}/train')    \n",
    "    val_cache_dir = os.path.join(data_rpath,f'cachingDataset/{file_prefixfold}/val')\n",
    "    \n",
    "    is_done_train = removeAndcreate_cachedir(train_cache_dir)\n",
    "    is_done_val = removeAndcreate_cachedir(val_cache_dir)\n",
    "    \n",
    "\n",
    "    n_train_cache_n_trans = len(train_transforms) #15\n",
    "    n_val_cache_n_trans = len(val_transforms)\n",
    "    \n",
    "     # create a training data loader\n",
    "\n",
    "    train_dataset = monai.data.CacheNTransDataset(data=train_files, transform=train_transforms,\\\n",
    "                                                cache_n_trans = n_train_cache_n_trans, cache_dir = train_cache_dir)\n",
    "    #train_folds['fold0_IDH_label']\n",
    "    uval, ucnt = np.unique(train_files_IDH_label, return_counts=True)\n",
    "    weight = 1./(ucnt/ucnt.min())\n",
    "    #weight = 1./ucnt\n",
    "    #weight = np.array([0.55, 0.45])\n",
    "    sample_weights = np.array([weight[int(t)] for t in train_files_IDH_label])\n",
    "    sample_weights = torch.from_numpy(sample_weights)\n",
    "    sampler = WeightedRandomSampler(sample_weights, num_samples= len(sample_weights), replacement=True)\n",
    "\n",
    "\n",
    "    #train_dataset = Dataset(data=train_files, transform=train_transforms)\n",
    "    #train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True, num_workers=2, pin_memory=True)\n",
    "    #train_dataset = CacheDataset(data=train_files, transform=train_transforms, cache_rate = 1.0, num_workers=8)\n",
    "    #train_loader = ThreadDataLoader(train_dataset, num_workers=0, batch_size=batch_size, shuffle=True)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=False, num_workers=1, sampler = sampler)\n",
    "    \n",
    "    # create a validation data loader\n",
    "    \n",
    "    #val_dataset = Dataset(data=val_files, transform=val_transforms)\n",
    "    #val_loader = DataLoader(val_dataset, batch_size=1, shuffle=False, num_workers=2, pin_memory=True)\n",
    "    #val_dataset = CacheDataset(data=val_files, transform=val_transforms, cache_rate = 1.0, num_workers=5)\n",
    "    \n",
    "    val_dataset = monai.data.CacheNTransDataset(data=val_files, transform=val_transforms,\\\n",
    "                                            cache_n_trans = n_val_cache_n_trans, cache_dir = val_cache_dir)\n",
    "    #val_loader = ThreadDataLoader(val_dataset, num_workers=0, batch_size=1)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=1, shuffle=False) #num_workers=2, pin_memory=True\n",
    "    \n",
    "    \n",
    "    \n",
    "#     for ibatch in train_loader:\n",
    "#         ibatch_IDH = ibatch['IDH_label']\n",
    "#         print(torch.eq(ibatch_IDH, 0).sum(), torch.eq(ibatch_IDH, 1).sum())\n",
    "#         print('#'*50)\n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "    just initialising some basic steps\n",
    "    \"\"\"\n",
    "    \n",
    "    max_epochs = epochs\n",
    "    find_lr=False\n",
    "    \n",
    "    ### Calling the loss function ***CrossEntropyPlusMSELoss**,and optimizer   \n",
    "    #loss_function = nn.CrossEntropyLoss()\n",
    "    #loss_function=nn.BCEWithLogitsLoss()\n",
    "    \n",
    "    #optimizer = torch.optim.Adam(model.parameters(), lr=1e-03, weight_decay=1e-07)\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr = 1e-05, weight_decay = 1e-4)\n",
    "    #optimizer = torch.optim.SGD(model.parameters(), lr=1e-03, momentum= 0.99, nesterov=True)\n",
    "    #optimizer = torch.optim.Adam(model.parameters(), 1e-03, weight_decay=1e-04)\n",
    "    scaler = torch.cuda.amp.GradScaler()\n",
    "    \n",
    "    \n",
    "    max_lr_init = 1e-03\n",
    "    \"\"\"\n",
    "     ###################### Block for LR finder from pytorch ignite ########################\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    if find_lr:\n",
    "\n",
    "        def prepare_batch(batch, device=None, non_blocking=False):\n",
    "            return _prepare_batch((batch['image'], batch['IDH_label'].long()), device, non_blocking)\n",
    "\n",
    "        #trainer = create_supervised_trainer(model, optimizer, loss_function, device, non_blocking=False, prepare_batch=prepare_batch)\n",
    "        def train_step(engine, batch):\n",
    "            model.train()\n",
    "            optimizer.zero_grad()\n",
    "            x, y = batch['image'].to(device), batch['IDH_label'].to(device)  #non_blocking=True\n",
    "            with torch.cuda.amp.autocast():\n",
    "                y_pred = model(x)\n",
    "                loss4lr = loss_function(y_pred, y)\n",
    "                \n",
    "            scaler.scale(loss4lr).backward()\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            return loss4lr.item()\n",
    "\n",
    "        trainer = Engine(train_step)\n",
    "\n",
    "        ProgressBar(persist=True).attach(trainer, output_transform=lambda x: {\"batch loss\": x})\n",
    "        lr_finder = FastaiLRFinder()\n",
    "        to_save={'model': model, 'optimizer': optimizer}\n",
    "        num_iter = 100  #2*len(train_loader)\n",
    "        run_epochs = int(np.ceil(num_iter/len(train_loader)))\n",
    "        with lr_finder.attach(trainer, to_save, end_lr=10, num_iter=num_iter, diverge_th=1.5) as trainer_with_lr_finder:    ####diverge_th=1.5\n",
    "\n",
    "            trainer_with_lr_finder.run(train_loader, max_epochs=run_epochs)  #max_epochs=run_epochs or 5\n",
    "\n",
    "        ax = lr_finder.plot()\n",
    "        plt.show()\n",
    "        \n",
    "        max_lr = lr_finder.lr_suggestion() if lr_finder.lr_suggestion()<5e-03 else max_lr_init\n",
    "        #max_lr = lr_finder.lr_suggestion() ##max_lr/10 i guess not needed, ignite does itself\n",
    "        print(f'Suggested learning rate by LR finder for this fold: {lr_finder.lr_suggestion()}')\n",
    "        \n",
    "    else:\n",
    "        max_lr = max_lr_init\n",
    "        \n",
    "    #max_lr_slice = 1e01*max_lr if max_lr<5e-03 else 1e-02\n",
    "    #max_lr_slice = 1e-01*max_lr if max_lr<1e-05 else max_lr\n",
    "        \n",
    "    \n",
    "    \"\"\"\n",
    "    ### defining learning rate scheduler\n",
    "    \n",
    "    \"\"\"\n",
    "    #steps_per_epoch=len(train_loader)\n",
    "    #optimizer.param_groups[0]['lr'] = max_lr #*1e-01\n",
    "    #scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr=max_lr_slice, steps_per_epoch=len(train_loader), epochs=max_epochs)\n",
    "    #scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda=lambda epoch: (1 - epoch / max_epochs) ** 0.9)\n",
    "    \n",
    "    #max_lr = 1e-3   \n",
    "    optimizer = Ranger21(model.parameters(), lr = max_lr, num_epochs = epochs, num_batches_per_epoch = len(train_loader))\n",
    "    \n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "     ###################### Block for native pytorch training loop and  ########################\n",
    "    \"\"\"\n",
    "\n",
    "    key_metric_n_saved = 1   ### Usually I keep it 5\n",
    "    save_last = False \n",
    "    dispformat_specs = '.4f'\n",
    "\n",
    "        \n",
    "#     file_prefix = 'ConvEffNet_Brats21_5CV'\n",
    "#     savedirname = 'ConvEffNet_Brats21'\n",
    "#     save_dir = os.path.join('/raid/brats2021/pthBraTS2021Radiogenomics', savedirname)\n",
    "#     if not os.path.exists(save_dir):\n",
    "#         os.makedirs(save_dir)\n",
    "\n",
    "    logsfile_path = f\"{save_dir}/Logs_{file_prefix}.txt\"\n",
    "\n",
    "\n",
    "    epoch_num = max_epochs #  max_epochs\n",
    "    val_interval = 1\n",
    "    valstep = 0\n",
    "    best_metric = -1\n",
    "    best_metric_epoch = -1\n",
    "    epoch_loss_values = list()\n",
    "    metric_values = list()\n",
    "\n",
    "\n",
    "    numsiters = len(train_files) // train_loader.batch_size\n",
    "\n",
    "    first_batch = monai.utils.misc.first(train_loader)\n",
    "        \n",
    "    \n",
    "    #post_pred = AsDiscrete(argmax=True, to_onehot=num_classes)  ### num_classes=num_classes\n",
    "    #post_label = AsDiscrete(to_onehot=num_classes) ###num_classes=num_classes\n",
    "    #dice_metric = monai.metrics.DiceMetric(include_background=False, reduction='mean', get_not_nans=False)\n",
    "    \n",
    "    \n",
    "    dice_metric = monai.metrics.DiceMetric(include_background=True, reduction='mean', get_not_nans=False)\n",
    "    dice_metric_batch = monai.metrics.DiceMetric(include_background=True, reduction='mean_batch', get_not_nans=False)\n",
    "    post_pred = Compose([Activations(sigmoid=True), AsDiscrete(threshold=0.5)])  \n",
    "    \n",
    "    def one_hot_permute(x):\n",
    "        return F.one_hot(x.squeeze(dim=0).long(), num_classes=num_classes).permute(3, 0, 1, 2)\n",
    "    \n",
    "    def get_binarize_tensor(x, dim=1):\n",
    "        x_chlist = torch.unbind(x, dim = dim)\n",
    "        bin_x = torch.zeros_like(x_chlist[0])\n",
    "        for x_i in x_chlist:\n",
    "            bin_x = torch.logical_or(x_i, bin_x)\n",
    "        return bin_x.unsqueeze(dim=dim).to(torch.float32)\n",
    "    \n",
    "    def get_segclass(x, dim = 1):\n",
    "        xdvc = x.device\n",
    "        x_chlist = torch.unbind(x, dim = dim)\n",
    "        xclassNoList = []\n",
    "        for x_i in x_chlist:\n",
    "            xv, xc = torch.unique(x_i, return_counts  = True)\n",
    "            \n",
    "            if torch.any(torch.eq(xv, 1)):\n",
    "                xclassNoList.append(xc[1].item())\n",
    "            else:\n",
    "                xclassNoList.append(0)\n",
    "        xclass = torch.argmax(torch.tensor(xclassNoList).to(xdvc))          \n",
    "        return xclass \n",
    "                \n",
    "            \n",
    "        \n",
    "            \n",
    "    \n",
    "    for epoch in range(epoch_num):\n",
    "        print(\"-\" * 10)\n",
    "        print(f\"epoch {epoch + 1}/{epoch_num}\")\n",
    "        model.train()\n",
    "        epoch_loss = 0.\n",
    "        stepiter = 0\n",
    "        for batch_data in train_loader:\n",
    "            stepiter += 1\n",
    "            inputs, labels, IDH_labels= (\n",
    "                batch_data['image'].to(device),\n",
    "                batch_data['label'].to(device),\n",
    "                batch_data['IDH_label'].to(device),\n",
    "            )\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            with torch.cuda.amp.autocast():\n",
    "\n",
    "                # compute output\n",
    "                outputs  = model(inputs)\n",
    "                loss = loss_function(outputs, labels) \n",
    "\n",
    "            \n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "            print(f\"{stepiter}/{numsiters}, train_loss: {loss.item():.4f}\")\n",
    "\n",
    "            #scheduler.step() \n",
    "            \n",
    "        epoch_loss /= stepiter\n",
    "        epoch_loss_values.append(epoch_loss)\n",
    "        print(f\"epoch {epoch + 1} average loss: {epoch_loss:.4f}\")\n",
    "        \n",
    "        \n",
    "        if (epoch + 1) % val_interval == 0:\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "\n",
    "                y_pred = torch.tensor([], dtype=torch.float32, device=device)\n",
    "                y = torch.tensor([], dtype=torch.long, device=device)\n",
    "                val_losses = torch.tensor([], dtype=torch.float32, device=device)\n",
    "\n",
    "\n",
    "                for val_data in val_loader:\n",
    "\n",
    "                    val_inputs, val_labels, val_IDH_labels = (\n",
    "                        val_data['image'].to(device),\n",
    "                        val_data['label'].to(device),\n",
    "                        val_data['IDH_label'].to(device),\n",
    "                    )\n",
    "                \n",
    "                    #roi_size = patch_size #(32, 32, 32)\n",
    "                    #sw_batch_size = 1\n",
    "                    #overlap = 0.25\n",
    "                    \n",
    "                    with torch.cuda.amp.autocast():\n",
    "                        \n",
    "                        #val_outputsC = sliding_window_inference_classes(val_inputs, roi_size, sw_batch_size, model, overlap=overlap, sw_device = device, device = device)\n",
    "                        val_outputs = model(val_inputs)\n",
    "                        val_ce_loss = loss_function(val_outputs.unsqueeze(dim=1), val_labels)\n",
    "\n",
    "                    val_losses = torch.cat([val_losses, val_ce_loss.view(1)], dim = 0)\n",
    "                    val_outputs = torch.stack([post_pred(i) for i in torch.unbind(val_outputs, dim = 0)], dim = 0)\n",
    "                    \n",
    "                    \n",
    "                    #val_labels2hot = torch.stack([one_hot_permute(i) for i in torch.unbind(val_labels, dim = 0)], dim = 0)\n",
    "\n",
    "                    \n",
    "                    val_labels_bin = get_binarize_tensor(val_labels, dim=1)\n",
    "                    val_outputs_bin = get_binarize_tensor(val_outputs, dim=1)\n",
    "                    \n",
    "                    dice_metric(y_pred=val_outputs_bin, y=val_labels_bin)\n",
    "                    \n",
    "                    \n",
    "                    klcc = KeepLargestConnectedComponent(applied_labels = [0, 1])  ##is_onehot=True\n",
    "                    #val_labels = klcc(val_labels.squeeze(dim=0)).unsqueeze(dim=0)\n",
    "                    val_outputs = klcc(val_outputs.squeeze(dim=0)).unsqueeze(dim=0)\n",
    "                    \n",
    "                    val_label4mSeg_C = get_segclass(val_outputs)\n",
    "                    #val_surv_labels = val_surv_labels.squeeze(dim=1)  ###Squeezing from B, 1 to B if needed\n",
    "                    y_pred = torch.cat([y_pred, val_label4mSeg_C.view(1)], dim=0)\n",
    "                    y = torch.cat([y, val_IDH_labels], dim=0)\n",
    "\n",
    "                mdice_value = dice_metric.aggregate()\n",
    "                dice_metric.reset()\n",
    "                \n",
    "                y_pred, y = y_pred.cpu(), y.cpu()\n",
    "                acc_value = torch.eq(y_pred, y)\n",
    "                acc_metric = acc_value.sum().item() / len(acc_value)\n",
    "                \n",
    "                #y_onehot = [post_label(i) for i in torch.unbind(y, dim=0)]\n",
    "                #y_pred_act = [post_pred(i) for i in torch.unbind(y_pred, dim=0)]\n",
    "                \n",
    "                auc_metric(y_pred, y)\n",
    "                auc_result = auc_metric.aggregate()\n",
    "                auc_metric.reset()\n",
    "            \n",
    "                accscore = accuracy_score(y, y_pred)\n",
    "                f1score = f1_score(y, y_pred, average='micro')\n",
    "                del y, y_pred\n",
    "                \n",
    "            \n",
    "                epoch_val_losses=torch.mean(val_losses).detach().cpu().item()\n",
    "                metric = auc_result\n",
    "                mdice_value = mdice_value.item()\n",
    "                #metric = mdice_value\n",
    "                metric_values.append(metric) ######List of over number of epochs\n",
    "                printstring = \"Best AUC\"\n",
    "                \n",
    "\n",
    "                with open(logsfile_path, 'a') as file:\n",
    "                    file.write(\n",
    "                        f\"current fold: {cfold} current epoch: {epoch + 1} dice_score: {mdice_value:^{dispformat_specs}} acc_metric: {auc_result:^{dispformat_specs}}\" \n",
    "                        f\" accuracy: {accscore:^{dispformat_specs}}, f1score: {f1score:^{dispformat_specs}}\"\n",
    "                        f\" epoch {epoch + 1} average training loss: {epoch_loss:^{dispformat_specs}} average validation loss: {epoch_val_losses:^{dispformat_specs}} \\n\"\n",
    "\n",
    "                    )\n",
    "\n",
    "                if valstep < key_metric_n_saved:\n",
    "                    torch.save(model.state_dict(), os.path.join(save_dir, f\"{file_prefix}_{metric:^{dispformat_specs}}_Fold{cfold}_epoch{epoch + 1}.pth\"))\n",
    "                    print(\n",
    "                        f\"current fold: {cfold} current epoch: {epoch + 1} dice_score: {mdice_value:^{dispformat_specs}} acc_metric: {auc_result:^{dispformat_specs}}\" \n",
    "                        f\" accuracy: {accscore:^{dispformat_specs}}, f1score: {f1score:^{dispformat_specs}}\"\n",
    "                        f\" epoch {epoch + 1} average training loss: {epoch_loss:^{dispformat_specs}} average validation loss: {epoch_val_losses:^{dispformat_specs}}\"\n",
    "                        \n",
    "                    )\n",
    "\n",
    "                else:\n",
    "\n",
    "                    #sortmetric_values = sorted(metric_values[:-1], reverse=True)  ###Higher loss needs to be deleted, so sorting is reversed\n",
    "                    sortmetric_values = sorted(metric_values[:-1], reverse=False)  \n",
    "\n",
    "                    if metric>=sortmetric_values[-key_metric_n_saved]:\n",
    "                        savegood_metric = metric\n",
    "                        good_metric_epoch = epoch + 1\n",
    "\n",
    "                        #if os.path.exists(f\"{save_dir}/{file_prefix}_{sortmetric_values[-key_metric_n_saved]:.4f}.pth\"):\n",
    "                        #    os.remove(f\"{save_dir}/{file_prefix}_{sortmetric_values[-key_metric_n_saved]:.4f}.pth\")\n",
    "                        #else:\n",
    "                        #    print(\"The file does not exist\")\n",
    "\n",
    "                        glblist = glob.glob(f\"{save_dir}/{file_prefix}_{sortmetric_values[-key_metric_n_saved]:^{dispformat_specs}}_*\")\n",
    "\n",
    "                        if not glblist:\n",
    "                            print(\"The file does not exist\")\n",
    "                        else:\n",
    "                            os.remove(glblist[0])\n",
    "\n",
    "\n",
    "                        torch.save(model.state_dict(), os.path.join(save_dir, f\"{file_prefix}_{metric:^{dispformat_specs}}_Fold{cfold}_epoch{epoch + 1}.pth\"))\n",
    "                        print(\"saved new best metric model\")\n",
    "                        print(\n",
    "                            f\"current fold: {cfold} current epoch: {epoch + 1} validation loss: {epoch_val_losses:^{dispformat_specs}}\"\n",
    "                            f\" dice_score: {mdice_value:^{dispformat_specs}} acc_metric: {auc_result:^{dispformat_specs}}\"\n",
    "                            f\" accuracy: {accscore:^{dispformat_specs}}, f1score: {f1score:^{dispformat_specs}}\"\n",
    "                            f\"\\n saved {printstring}: {savegood_metric:^{dispformat_specs}} at epoch: {good_metric_epoch}\"\n",
    "                        )\n",
    "\n",
    "                    else:\n",
    "\n",
    "                        f\"current fold: {cfold} current epoch: {epoch + 1} validation loss: {epoch_val_losses:^{dispformat_specs}}\"\n",
    "                        f\" dice_score: {mdice_value:^{dispformat_specs}} acc_metric: {auc_result:^{dispformat_specs}}\"\n",
    "                        f\" accuracy: {accscore:^{dispformat_specs}}, f1score: {f1score:^{dispformat_specs}}\"\n",
    "\n",
    "                        #pass\n",
    "\n",
    "                valstep += 1\n",
    "        ####Saving last epoch\n",
    "        if epoch==epoch_num-1:\n",
    "            if save_last:\n",
    "                torch.save(model.state_dict(), os.path.join(save_dir, f\"{file_prefix}_{metric:^{dispformat_specs}}_Fold{cfold}_last_epoch{epoch + 1}.pth\"))\n",
    "            #break\n",
    "            \n",
    "    # Free up GPU memory after training\n",
    "    model = None\n",
    "    train_loader, val_loader = None, None\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "337399b6",
   "metadata": {},
   "source": [
    "### Loop to execute n_splits=3 fold cross validation\n",
    "if the model is trained and the checkpoints are saved already, just setting the start_training flag as false, to run remaining part of the programs of this notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "eff2a19a",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_training = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "6705ec82",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "n_splits = 3\n",
    "if start_training:\n",
    "    #### Running 10 folds\n",
    "    for i in range(0, n_splits):\n",
    "        \n",
    "        train_files_fld, train_files_fld_IDH_label, val_files_fld, val_files_fld_IDH_label  = train_folds[f'fold{i}'], train_folds[f'fold{i}_IDH_label'],\\\n",
    "        val_folds[f'fold{i}'], val_folds[f'fold{i}_IDH_label']\n",
    "        batch_size=16\n",
    "        ### Need to change batch size if minimux training batch size == 1\n",
    "        print('fold', i, \"Bacth Investigation, minimum batch size\", len(train_files_fld)%batch_size)\n",
    "        #train_files_fld, val_files_fld = sklearn_transforms(train_files_fld, val_files_fld)\n",
    "        train(train_files_fld, train_files_fld_IDH_label, val_files_fld, val_files_fld_IDH_label, batch_size=batch_size, epochs = 500, cfold = i)\n",
    "    \n",
    "    start_training = False\n",
    "else:\n",
    "    \n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f04cd68c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.datasets import make_multilabel_classification\n",
    "# from sklearn.multioutput import MultiOutputClassifier\n",
    "# from sklearn.linear_model import LogisticRegression\n",
    "# X, y = make_multilabel_classification(random_state=0, n_classes=2)\n",
    "# inner_clf = LogisticRegression(solver=\"liblinear\", random_state=0)\n",
    "# clf = MultiOutputClassifier(inner_clf).fit(X, y)\n",
    "# y_score = np.transpose([y_pred[:, 1] for y_pred in clf.predict_proba(X)])\n",
    "# roc_auc_score(y, y_score, average=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "42581320",
   "metadata": {},
   "outputs": [],
   "source": [
    "# roc_auc_score(y, y_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "60c983ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_permute(x):\n",
    "    return F.one_hot(x.squeeze(dim=0).long(), num_classes=3).permute(2, 0, 1)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "4c5dab51",
   "metadata": {},
   "source": [
    "atensor = torch.tensor([[[0, 2, 0, 0],[0, 0, 0, 2],[0, 2, 0, 0]]])\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "89a17ee3",
   "metadata": {},
   "source": [
    "one_hot_permute(atensor)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "3a8d7a3c",
   "metadata": {},
   "source": [
    "atensor = torch.tensor([[[0, 1, 0, 0],[0, 0, 0, 1],[0, 1, 0, 0]]])\n",
    "one_hot_permute(atensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "e34a37d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "xc= torch.tensor([0, 1, 0, 0, 2, 3, 10])\n",
    "if torch.any(torch.eq(xc, 11)):\n",
    "    print('Do')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "b4115811",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(3)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xc = torch.tensor([0, 100, 500, 10000, 5])\n",
    "torch.argmax(xc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "189fe525",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(3)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.argsort(xc)[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "662d0001",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "is_onehot = True\n",
    "img = torch.ones((1, 64, 64, 64))\n",
    "is_onehot2 = img.shape[0] > 1 if is_onehot is None else is_onehot\n",
    "is_onehot2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cb13f9d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img.shape[0] > 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b2be5a5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
